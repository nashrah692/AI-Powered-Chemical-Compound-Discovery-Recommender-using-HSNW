{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11487321,"sourceType":"datasetVersion","datasetId":7200232},{"sourceId":11944874,"sourceType":"datasetVersion","datasetId":7509207}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div>\n    <h1 align=\"center\"><font color=\"blue\"> DELIVERABLE 2 </font></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Downloading Libraries </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"pip install rdkit-pypi torch_geometric faiss-cpu sacremoses --quiet","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab":{"base_uri":"https://localhost:8080/"},"id":"dXsSaIFHPEAc","outputId":"5667d7dd-a341-4c9e-a923-38a5ae4af8c8","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:20:28.366114Z","iopub.execute_input":"2025-06-08T17:20:28.366403Z","iopub.status.idle":"2025-06-08T17:20:36.636762Z","shell.execute_reply.started":"2025-06-08T17:20:28.366382Z","shell.execute_reply":"2025-06-08T17:20:36.635851Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# rdkit-pypi: Helps me work with chemical structures and SMILES strings for molecules.\n# torch_geometric: Allows me to build graph neural networks (GNNs) for processing molecular data.\n# faiss-cpu: Used for fast similarity searches with embeddings, like finding similar compounds.\n# sacremoses: Likely needed for text processing, possibly for the language model part.\n# bitsandbytes: Helps with memory-efficient model training, especially for large language models.\n                                             \nprint(\"---------- ALL LIBRARIES HAVE BEEN DOWNLOADED ----------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:20:36.638067Z","iopub.execute_input":"2025-06-08T17:20:36.638258Z","iopub.status.idle":"2025-06-08T17:20:36.642585Z","shell.execute_reply.started":"2025-06-08T17:20:36.638238Z","shell.execute_reply":"2025-06-08T17:20:36.641911Z"}},"outputs":[{"name":"stdout","text":"---------- ALL LIBRARIES HAVE BEEN DOWNLOADED ----------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Importing Libraries </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINConv, global_add_pool\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.loader import DataLoader\nimport numpy as np\nimport pandas as pd\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, DataStructs\n\nimport faiss\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, PretrainedConfig\nfrom rdkit.Chem import Descriptors\nfrom tqdm import tqdm\nimport gc\nimport os\nimport ast\nimport re\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.amp import GradScaler, autocast\nfrom sklearn.model_selection import train_test_split\nfrom rdkit import RDLogger\n\nprint(\"---------- ALL LIBRARIES HAVE BEEN IMPORTED ----------\")\n\n# torch, torch.nn, and torch.nn.functional: For building and training neural networks, like my GNN model.\n# torch_geometric modules (GINConv, global_add_pool, Data, Batch, DataLoader): Help me create and process graph-based data for molecules.\n# rdkit modules (Chem, AllChem, DataStructs, Descriptors): lets me work with chemical structures, generate fingerprints, and calculate properties like logP.\n# faiss: For efficient similarity searches using embeddings.\n# transformers modules (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig): For loading and using large language models (LLMs) like BioMistral.\n# tqdm: Adds progress bars to loops, so I can see how long processes take.\n# gc: Helps manage memory by cleaning up unused objects.\n# ast and re: For parsing strings and extracting information from text, like LLM outputs.\n# torch.cuda.amp (GradScaler, autocast): Optimizes training on GPUs to save memory and speed up computations.\n# bitsandbytes: Reduces memory usage for LLMs.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:20:36.643546Z","iopub.execute_input":"2025-06-08T17:20:36.643781Z","iopub.status.idle":"2025-06-08T17:20:53.834080Z","shell.execute_reply.started":"2025-06-08T17:20:36.643750Z","shell.execute_reply":"2025-06-08T17:20:53.833334Z"}},"outputs":[{"name":"stdout","text":"---------- ALL LIBRARIES HAVE BEEN IMPORTED ----------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"<div>\n    <h2 align=\"center\"><font color=\"purple\"> Deliverable 1 Code </font></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Set device for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:20:53.835915Z","iopub.execute_input":"2025-06-08T17:20:53.836349Z","iopub.status.idle":"2025-06-08T17:20:53.840760Z","shell.execute_reply.started":"2025-06-08T17:20:53.836291Z","shell.execute_reply":"2025-06-08T17:20:53.839991Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 01: Data Loading and Preprocessing </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/smiles/SMILES_Big_Data_Set.csv')\nprint(\"Dataset columns:\", df.columns.tolist())\n\n# Standardizing SMILES strings to ensure consistency and track invalid ones.\ninvalid_smiles_count = 0\ndef standardize_smiles(smiles):\n    global invalid_smiles_count\n    try:\n        mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object.\n        if mol is None:\n            invalid_smiles_count += 1 \n            return None\n        return Chem.MolToSmiles(mol, isomericSmiles=True)  # Convert back to standardized SMILES.\n    except:\n        invalid_smiles_count += 1  # Increment counter if conversion fails.\n        return None\n\ndf['standard_smiles'] = df['SMILES'].apply(standardize_smiles) \ndf = df.dropna(subset=['standard_smiles']).drop_duplicates(subset=['standard_smiles'])\nprint(f\"Removed {invalid_smiles_count} invalid SMILES strings.\")\n\n\ndf['pIC50'] = pd.to_numeric(df['pIC50'], errors='coerce') \ndf['num_atoms'] = pd.to_numeric(df['num_atoms'], errors='coerce')  \ndf['logP'] = pd.to_numeric(df['logP'], errors='coerce') \ndf = df.dropna() \n\n# Creating a column of RDKit molecule objects for later use, like generating fingerprints.\ndf['mol'] = df['standard_smiles'].apply(Chem.MolFromSmiles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:20:53.842083Z","iopub.execute_input":"2025-06-08T17:20:53.842711Z","iopub.status.idle":"2025-06-08T17:20:58.587320Z","shell.execute_reply.started":"2025-06-08T17:20:53.842691Z","shell.execute_reply":"2025-06-08T17:20:58.586562Z"}},"outputs":[{"name":"stdout","text":"Dataset columns: ['SMILES', 'pIC50', 'mol', 'num_atoms', 'logP']\nRemoved 0 invalid SMILES strings.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 02: Generating Fingerprints (Morgan Fingerprints) </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Creating Morgan fingerprints to represent molecular structures numerically for GNN input.\ndef generate_morgan_fingerprint(mol, radius=2, n_bits=2048):\n    if mol is None:\n        return None\n    try:\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)  # Generate 2048-bit Morgan fingerprint with radius 2.\n        arr = np.zeros((n_bits,), dtype=np.float32)\n        DataStructs.ConvertToNumpyArray(fp, arr)  # Convert fingerprint to NumPy array of 0s and 1s.\n        return arr\n    except:\n        return None\n\ndf['morgan_fp'] = df['mol'].apply(generate_morgan_fingerprint)  \ndf = df[df['morgan_fp'].notnull()]  # Remove rows where fingerprint generation failed.\nfp_matrix = np.stack(df['morgan_fp'].values)  # Stack all fingerprints into a single NumPy array for GNN training.\nprint(f\"Fingerprint matrix shape: {fp_matrix.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-06-08T17:20:58.588154Z","iopub.execute_input":"2025-06-08T17:20:58.588461Z","iopub.status.idle":"2025-06-08T17:21:00.603788Z","shell.execute_reply.started":"2025-06-08T17:20:58.588436Z","shell.execute_reply":"2025-06-08T17:21:00.602974Z"},"id":"iHGFFs4kPEAd","outputId":"152d69c8-405a-4e3d-fb0e-36047bb5b921","trusted":true},"outputs":[{"name":"stdout","text":"Fingerprint matrix shape: (14823, 2048)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 03: GNN for Fingerprint Embedding (GIN) </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Defining a Graph Neural Network (GNN) to create compact embeddings from Morgan fingerprints.\nclass FingerprintGNN(nn.Module):\n    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=256):\n        super().__init__()\n        self.fp_to_node = nn.Linear(input_dim, hidden_dim)  # Reduce 2048-bit fingerprint to 512 dimensions.\n        self.conv1 = GINConv(nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),  # First linear layer for graph convolution.\n            nn.ReLU(),  # Activation\n            nn.Linear(hidden_dim, hidden_dim)  # Second linear layer for feature transformation.\n        ))\n        self.conv2 = GINConv(nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),  # Second graph convolution layer.\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        ))\n        self.lin = nn.Linear(hidden_dim, output_dim)  # Final layer to output 256-dimensional embedding.\n\n    def forward(self, x, edge_index, batch):\n        x = self.fp_to_node(x)  # Transform input fingerprint to hidden dimension.\n        x = self.conv1(x, edge_index).relu() \n        x = self.conv2(x, edge_index) \n        pooled = global_add_pool(x, batch)  # Aggregate node features into a single embedding per graph.\n        return self.lin(pooled)  \n\ndata_list = []\nfor fp in df['morgan_fp']:\n    node_feat = torch.FloatTensor(fp).unsqueeze(0)  # Convert fingerprint to tensor and add batch dimension.\n    edge_index = torch.tensor([[0], [0]], dtype=torch.long) \n    data = Data(x=node_feat, edge_index=edge_index) \n    data_list.append(data)\n\nbatch_size = 128  # Set batch size for efficient training.\nloader = DataLoader(data_list, batch_size=batch_size, shuffle=False)  # Create DataLoader for batching graphs.\n\n# Training the GNN model using an autoencoder-like loss.\ngin_model = FingerprintGNN().to(device)  \noptimizer = torch.optim.Adam(gin_model.parameters(), lr=0.001)  # Set up Adam optimizer.\ntarget_projection = nn.Linear(2048, 256).to(device)  # Linear layer to project fingerprints to 256 dimensions for loss calculation.\n\n# Ensure=ing target_projection parameters are optimized along with GNN.\ncombined_params = list(gin_model.parameters()) + list(target_projection.parameters())\noptimizer = torch.optim.Adam(combined_params, lr=0.001) \n\nepochs = 10\n\nprint(\"\\nTraining GIN model...\")\nfor epoch in range(epochs):\n    gin_model.train()  \n    target_projection.train() \n    total_loss = 0\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = gin_model(batch.x, batch.edge_index, batch.batch)  # Get GNN embeddings.\n        target = target_projection(batch.x) \n        loss = F.mse_loss(out, target)  # Calculate MSE loss between GNN and projected embeddings.\n        loss.backward()\n        optimizer.step()  # Update model weights.\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader)}\")\n\n# Generating embeddings for all fingerprints using the trained GNN.\nprint(\"\\nGenerating GNN embeddings...\")\ngin_model.eval()  \ntarget_projection.eval() \nembeddings = []\nwith torch.no_grad():  # Disable gradient tracking to save memory.\n    for batch in loader:\n        batch = batch.to(device) \n        emb = gin_model(batch.x, batch.edge_index, batch.batch)  # Generate embeddings.\n        embeddings.append(emb.cpu().numpy()) \nembedding_matrix = np.vstack(embeddings) \nprint(f\"Embedding matrix shape: {embedding_matrix.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:21:00.604703Z","iopub.execute_input":"2025-06-08T17:21:00.604971Z","iopub.status.idle":"2025-06-08T17:21:09.977699Z","shell.execute_reply.started":"2025-06-08T17:21:00.604947Z","shell.execute_reply":"2025-06-08T17:21:09.977058Z"}},"outputs":[{"name":"stdout","text":"\nTraining GIN model...\nEpoch 1, Loss: 0.001448280632130577\nEpoch 2, Loss: 0.00047374526560627696\nEpoch 3, Loss: 0.0003846172364539033\nEpoch 4, Loss: 0.0003816857390810238\nEpoch 5, Loss: 0.0007722037550812619\nEpoch 6, Loss: 0.0018076886892607756\nEpoch 7, Loss: 0.0021057090990983978\nEpoch 8, Loss: 0.0012409834623746253\nEpoch 9, Loss: 0.0007501794311322336\nEpoch 10, Loss: 0.0005337967943040863\n\nGenerating GNN embeddings...\nEmbedding matrix shape: (14823, 256)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Saving preprocessed data, embeddings, trained model </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"# Saving my processed data and trained GNN model for later use.\ndf['gnn_embedding'] = embedding_matrix.tolist() \ndf.to_csv('preprocessed_data_with_embeddings.csv', index=False) \n\n# Saving the GNN model's weights to a file.\ntorch.save(gin_model.state_dict(), \"gin_model.pth\") \n\nprint(\"Data Saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:21:09.978443Z","iopub.execute_input":"2025-06-08T17:21:09.978715Z","iopub.status.idle":"2025-06-08T17:21:16.392081Z","shell.execute_reply.started":"2025-06-08T17:21:09.978695Z","shell.execute_reply":"2025-06-08T17:21:16.391455Z"}},"outputs":[{"name":"stdout","text":"Data Saved!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Checking if required columns exist in df </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"# Checking if my DataFrame has the necessary columns for later steps.\nif 'gnn_embedding' not in df.columns or 'standard_smiles' not in df.columns:\n    raise ValueError(\"Required columns 'gnn_embedding' or 'standard_smiles' not found in DataFrame.\")\nelse:\n    print(\"Required Columns Exist!\")\n\n# Resetting the DataFrame index to align with the embedding matrix.\ndf = df.reset_index(drop=True)  # Ensure row indices match embedding matrix to avoid mismatches.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:21:16.392689Z","iopub.execute_input":"2025-06-08T17:21:16.392889Z","iopub.status.idle":"2025-06-08T17:21:16.401102Z","shell.execute_reply.started":"2025-06-08T17:21:16.392873Z","shell.execute_reply":"2025-06-08T17:21:16.400537Z"}},"outputs":[{"name":"stdout","text":"Required Columns Exist!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 04: HNSW Index for GNN Embeddings </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Converting GNN embeddings to a NumPy array for Faiss.\nembedding_matrix = np.stack(df['gnn_embedding'].values).astype(np.float32)  \nembedding_dim = embedding_matrix.shape[1] \n\nindex = faiss.IndexHNSWFlat(embedding_dim, 32)  # Create HNSW index with M=32 (graph degree).\nindex.hnsw.efConstruction = 200  # Set construction parameter for better index quality.\nindex.hnsw.efSearch = 100  # Set search parameter for better accuracy.\nfaiss.normalize_L2(embedding_matrix)  # Normalize embeddings for cosine similarity.\n\nindex.add(embedding_matrix)  # Index all embeddings for similarity searches.\nprint(f\"Indexed {embedding_matrix.shape[0]} compounds.\")\n\n# Saving the index to a file for later use.\nfaiss.write_index(index, \"gnn_hnsw_index.faiss\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:21:16.403674Z","iopub.execute_input":"2025-06-08T17:21:16.403906Z","iopub.status.idle":"2025-06-08T17:21:47.765869Z","shell.execute_reply.started":"2025-06-08T17:21:16.403887Z","shell.execute_reply":"2025-06-08T17:21:47.765086Z"}},"outputs":[{"name":"stdout","text":"Indexed 14823 compounds.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 05: HNSW Search Function </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Defining a function to find compounds similar to a query fingerprint using the HNSW index.\ndef search_similar_compounds(query_fp, gin_model, index, top_k=5, device='cpu'):\n    \"\"\"\n    Search for compounds similar to the query fingerprint using HNSW index.\n    \"\"\"\n    try:\n        # Setting up the GNN model to generate embeddings for the query.\n        gin_model.eval() \n        gin_model.to(device) \n\n        query_fp = np.array(query_fp, dtype=np.float32)  \n        node_feat = torch.FloatTensor(query_fp).unsqueeze(0).to(device) \n        edge_index = torch.tensor([[0], [0]], dtype=torch.long).to(device)  # Create self-loop for single-node graph.\n        data = Data(x=node_feat, edge_index=edge_index)  # Wrap in Data object.\n        batch = torch.zeros(1, dtype=torch.long).to(device)  # Batch tensor for single graph.\n\n        with torch.no_grad(): \n            query_embedding = gin_model(data.x, data.edge_index, batch).cpu().numpy()  # Get 256-dimensional embedding.\n        \n        query_embedding = query_embedding.astype(np.float32) \n        faiss.normalize_L2(query_embedding)\n\n        # Searching for the top_k most similar compounds.\n        _, indices = index.search(query_embedding, top_k)  \n\n        # Retrieving the SMILES strings of similar compounds.\n        similar_smiles = df.iloc[indices[0]]['standard_smiles'].values.tolist() \n        return similar_smiles\n    \n    except Exception as e:\n        print(f\"Error during similarity search: {e}\")\n        return []  \n\nprint(\"Similar Compound Search Function made!\")","metadata":{"execution":{"iopub.status.busy":"2025-06-08T17:21:47.766802Z","iopub.execute_input":"2025-06-08T17:21:47.767085Z","iopub.status.idle":"2025-06-08T17:21:47.774139Z","shell.execute_reply.started":"2025-06-08T17:21:47.767060Z","shell.execute_reply":"2025-06-08T17:21:47.773550Z"},"id":"gKO3krgLPEAe","outputId":"ec6c4291-c092-40d0-99eb-7331b4151e2f","trusted":true},"outputs":[{"name":"stdout","text":"Similar Compound Search Function made!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Example Search Using HNSW </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"print(\"\\nSearching for similar compounds...\")\n\n# Testing the similarity search with a sample SMILES string.\nquery_smiles = \"NS(=O)(=O)N1CCC(NC(=O)c2cnn3ccc(N4CCCC4c4cc(F)ccc4F)nc23)CC1\"\nquery_mol = Chem.MolFromSmiles(query_smiles)  # Convert SMILES to RDKit molecule.\nif query_mol is None:\n    print(\"Error: Invalid query SMILES string.\")\nelse:\n    query_fp = generate_morgan_fingerprint(query_mol)  # Generate Morgan fingerprint for query.\n    if query_fp is None:\n        print(\"Error: Failed to generate fingerprint for query molecule.\")\n    else:\n        # Using the search function to find similar compounds.\n        similar_compounds = search_similar_compounds(query_fp, gin_model, index, top_k=5, device=device)  # Find top 5 similar compounds.\n        print(\"\\nTop 5 Similar Compounds:\")\n        for i, smiles in enumerate(similar_compounds, 1):\n            print(f\"{i}. {smiles}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-08T17:21:47.774965Z","iopub.execute_input":"2025-06-08T17:21:47.775223Z","iopub.status.idle":"2025-06-08T17:21:47.811720Z","shell.execute_reply.started":"2025-06-08T17:21:47.775201Z","shell.execute_reply":"2025-06-08T17:21:47.810942Z"},"id":"T39I6YSRPEAf","outputId":"4e5f57a5-38c5-4a56-fa13-0e678033614e","trusted":true},"outputs":[{"name":"stdout","text":"\nSearching for similar compounds...\n\nTop 5 Similar Compounds:\n1. N#CC1CC(NCCO)CCC1n1cc(C(N)=O)c(Nc2ccc(S(=O)(=O)C(F)(F)F)cc2)n1\n2. Nc1ccc2c(c1)Cc1ccccc1-2\n3. O=C1C(=C2Nc3ccccc3C2=O)Nc2ccccc21\n4. Nc1ncnc2ncn(C(c3ccccc3)c3ccccc3)c12\n5. COc1ccc(OC)c(Cc2cnc(N)nc2N)c1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"<div>\n    <h2 align=\"center\"><font color=\"purple\"> Deliverable 2 Code </font></h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Suppress Warnings </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"RDLogger.DisableLog('rdApp.*')\n\nprint(\"Suppress command executed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:21:47.812468Z","iopub.execute_input":"2025-06-08T17:21:47.812681Z","iopub.status.idle":"2025-06-08T17:21:47.816970Z","shell.execute_reply.started":"2025-06-08T17:21:47.812665Z","shell.execute_reply":"2025-06-08T17:21:47.816216Z"}},"outputs":[{"name":"stdout","text":"Suppress command executed!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Validating and Retrieving data from Deliverable 01 </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"# Loading preprocessed data from Deliverable 1\ndf = pd.read_csv('/kaggle/working/preprocessed_data_with_embeddings.csv')\nprint(\"Loaded columns:\", df.columns)\n\n# Validating required columns\nrequired_columns = ['standard_smiles', 'gnn_embedding']\nmissing = [col for col in required_columns if col not in df.columns]\nif missing:\n    raise ValueError(f\"Missing columns: {missing}\")\n\n# Loading FAISS HNSW index\nd = len(df['gnn_embedding'].iloc[0])  # Embedding dimension\nindex = faiss.read_index('/kaggle/working/gnn_hnsw_index.faiss')\nprint(\"HNSW index loaded with\", index.ntotal, \"embeddings\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:21:47.817825Z","iopub.execute_input":"2025-06-08T17:21:47.818083Z","iopub.status.idle":"2025-06-08T17:21:48.668683Z","shell.execute_reply.started":"2025-06-08T17:21:47.818039Z","shell.execute_reply":"2025-06-08T17:21:48.668014Z"}},"outputs":[{"name":"stdout","text":"Loaded columns: Index(['SMILES', 'pIC50', 'mol', 'num_atoms', 'logP', 'standard_smiles',\n       'morgan_fp', 'gnn_embedding'],\n      dtype='object')\nHNSW index loaded with 14823 embeddings\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Loading Pre-trained GIN Model </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"gin_model = FingerprintGNN().to(device)\ngin_model.load_state_dict(torch.load('/kaggle/working/gin_model.pth'))\ngin_model.eval()\n\nprint(\"Loaded pretrained GIN model successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:21:48.669376Z","iopub.execute_input":"2025-06-08T17:21:48.669561Z","iopub.status.idle":"2025-06-08T17:21:48.709261Z","shell.execute_reply.started":"2025-06-08T17:21:48.669546Z","shell.execute_reply":"2025-06-08T17:21:48.708504Z"}},"outputs":[{"name":"stdout","text":"Loaded pretrained GIN model successfully.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.2 (Step 01) </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Dictionary to store LLM configurations (only one active at a time)\nllm_configs = {\n    # 'BioGPT': {\n    #     'model_name': 'microsoft/biogpt',\n    #     'tokenizer': AutoTokenizer.from_pretrained('microsoft/biogpt'),\n    #     'model': AutoModelForCausalLM.from_pretrained('microsoft/biogpt').to(device)\n    # },\n    'MolT5': {\n        'model_name': 'laituan245/molt5-large-smiles2caption',\n        'tokenizer': AutoTokenizer.from_pretrained('laituan245/molt5-large-smiles2caption'),\n        'model': AutoModelForSeq2SeqLM.from_pretrained('laituan245/molt5-large-smiles2caption').to(device)\n    },\n    'ChemBERTa': {\n        'model_name': 'DeepChem/ChemBERTa-77M-MTR',\n        'tokenizer': AutoTokenizer.from_pretrained('DeepChem/ChemBERTa-77M-MTR'),\n        'model': AutoModelForSequenceClassification.from_pretrained('DeepChem/ChemBERTa-77M-MTR').to(device)\n    }\n}\nactive_llm = 'MolT5'  # Change to 'BioGPT', 'MolT5', or 'ChemBERTa' to switch models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:21:48.710139Z","iopub.execute_input":"2025-06-08T17:21:48.710416Z","iopub.status.idle":"2025-06-08T17:22:22.933143Z","shell.execute_reply.started":"2025-06-08T17:21:48.710393Z","shell.execute_reply":"2025-06-08T17:22:22.932121Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9258afb7f2424ee08360e68def3db73f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150706f5090c4650886a674133a94c49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b702f369c09840ca9afd4396567dee47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe96bf9a7c34947b8eb2ab8dbc3bbac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23ab4864d22481898337b2448785ac4"}},"metadata":{}},{"name":"stderr","text":"2025-06-08 17:21:52.717855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749403312.908164      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749403312.967117      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4cbe703e094fc486bd4ac7e08fc8a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a5093a2e17a4db98aad2b27d6bfd940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8641af8be3ae4c17be2c7bf61a1752c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594e369e762d43fdbe6087dbc81d6c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/6.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02be3e1c74124116a046f59dbd3f8602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5a244aa99d0450a89db3689fa9e2cb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/8.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"948b873dcba94ebcaf01bd912054343e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a762621ebc4b3fabd45c5817479cbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/420 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ac0978f9364dbc9465bacdf6a4e894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/14.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"247424692e844286b2a421b12216ced8"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.2 (Step 02) & R2.3 (Step 01) </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Set environment variable for CUDA memory optimization\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Fine-tune active LLM on SMILES dataset with diverse prompts\ndef fine_tune_llm(model, tokenizer, smiles_list, epochs=2, batch_size=1, accumulation_steps=4):\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n    scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n    # Determine model type using config\n    config = model.config\n    print(f\"Model config class: {config.__class__.__name__}\")  # Debug model type\n    if isinstance(config, PretrainedConfig) and hasattr(config, 'model_type'):\n        model_type = config.model_type\n        if model_type in ['biogpt', 'gpt2', 'llama']:  # Causal LM examples\n            model_type = 'causal'\n        elif model_type in ['t5', 'molt5']:  # Seq2Seq examples\n            model_type = 'seq2seq'\n        elif model_type in ['bert', 'roberta', 'chemberta']:  # Classification examples\n            model_type = 'classification'\n        else:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n    else:\n        raise ValueError(\"Unable to determine model type from configuration\")\n\n    # Create training examples with varied targets\n    templates = [\n        \"This compound, a potential drug candidate, may exhibit anti-inflammatory properties.\",\n        \"A novel structure for drug development with possible antimicrobial effects.\",\n        \"This chemical could be a new lead for cancer therapy research.\"\n    ]\n    train_data = [f\"{smiles}\\t{templates[i % len(templates)]}\" for i, smiles in enumerate(smiles_list)]\n\n    for epoch in range(epochs):\n        np.random.shuffle(train_data)  # Shuffle to improve learning\n        optimizer.zero_grad()\n        for i in tqdm(range(0, len(train_data), batch_size), desc=f\"Epoch {epoch+1}\"):\n            batch = train_data[i:i+batch_size]\n            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n            \n            with torch.cuda.amp.autocast():\n                if model_type == 'causal':\n                    outputs = model(**inputs, labels=inputs['input_ids'])\n                elif model_type == 'seq2seq':\n                    decoder_input_ids = inputs['input_ids'].clone()\n                    decoder_input_ids[:, 1:] = decoder_input_ids[:, :-1].clone()  # Shift for teacher forcing\n                    decoder_input_ids[:, 0] = tokenizer.pad_token_id  # Start with pad token\n                    outputs = model(**inputs, decoder_input_ids=decoder_input_ids, labels=inputs['input_ids'])\n                elif model_type == 'classification':\n                    labels = torch.zeros(len(batch), dtype=torch.long).to(device)  # Dummy labels\n                    outputs = model(**inputs, labels=labels)\n                loss = outputs.loss / accumulation_steps  # Scale loss for accumulation\n\n            scaler.scale(loss).backward()\n            if (i + 1) % accumulation_steps == 0 or i + 1 == len(train_data):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n    model.eval()\n\n# Generate recommendation using dataset-tuned LLM\ndef generate_dataset_tuned_recommendation(model, tokenizer, prompt):\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n    \n    if hasattr(model.config, 'model_type') and model.config.model_type in ['biogpt', 'gpt2', 'llama']:\n        outputs = model.generate(\n            **inputs,\n            max_length=200,\n            num_return_sequences=1,\n            do_sample=True,\n            top_k=40,\n            top_p=0.92,\n            temperature=0.8,\n            no_repeat_ngram_size=2\n        )\n    elif hasattr(model.config, 'model_type') and model.config.model_type in ['t5', 'molt5']:\n        outputs = model.generate(\n            **inputs,\n            max_length=200,\n            num_return_sequences=1,\n            do_sample=True,\n            top_k=40,\n            top_p=0.92,\n            temperature=0.8,\n            no_repeat_ngram_size=2,\n            decoder_start_token_id=tokenizer.pad_token_id\n        )\n    elif hasattr(model.config, 'model_type') and model.config.model_type in ['bert', 'roberta', 'chemberta']:\n        return \"ChemBERTa is a classification model and cannot generate recommendations directly.\"\n    else:\n        raise ValueError(f\"Unsupported model type for generation: {model.config.model_type if hasattr(model.config, 'model_type') else 'Unknown'}\")\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n\nprint(\"DATASET TUNING COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:22:22.934224Z","iopub.execute_input":"2025-06-08T17:22:22.935046Z","iopub.status.idle":"2025-06-08T17:22:22.952568Z","shell.execute_reply.started":"2025-06-08T17:22:22.935017Z","shell.execute_reply":"2025-06-08T17:22:22.951808Z"}},"outputs":[{"name":"stdout","text":"DATASET TUNING COMPLETE\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.4 </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Generate recommendation using Dataset\nsmiles_list = df['standard_smiles'].tolist()[:100]  # Reduced to 100 SMILES for memory\nprint(f\"Tuning on first 100 SMILES: {smiles_list[:5]}... (total {len(smiles_list)})\")  # Debug\nfine_tune_llm(llm_configs[active_llm]['model'], llm_configs[active_llm]['tokenizer'], smiles_list)\nprompt = \"Propose a novel chemical compound for drug development, including a SMILES string and its potential therapeutic application.\"\ndataset_tuned_rec = generate_dataset_tuned_recommendation(\n    llm_configs[active_llm]['model'], llm_configs[active_llm]['tokenizer'], prompt\n)\n\nprint(f\"Dataset-Tuned {active_llm} Recommendation: {dataset_tuned_rec}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:22:22.953512Z","iopub.execute_input":"2025-06-08T17:22:22.953902Z","iopub.status.idle":"2025-06-08T17:23:29.832397Z","shell.execute_reply.started":"2025-06-08T17:22:22.953883Z","shell.execute_reply":"2025-06-08T17:23:29.831723Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/14.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e980112b8844d16a8fd5dda168b396b"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/3085775951.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n","output_type":"stream"},{"name":"stdout","text":"Tuning on first 100 SMILES: ['O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1', 'O=c1cc(-c2nc(-c3ccc(-c4cn(CCP(=O)(O)O)nn4)cc3)[nH]c2-c2ccc(F)cc2)cc[nH]1', 'NC(=O)c1ccc2c(c1)nc(C1CCC(O)CC1)n2CCCO', 'NCCCn1c(C2CCNCC2)nc2cc(C(N)=O)ccc21', 'CNC(=S)Nc1cccc(-c2cnc3ccccc3n2)c1']... (total 100)\nModel config class: T5Config\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A/tmp/ipykernel_35/3085775951.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n\nEpoch 1:   1%|          | 1/100 [00:00<01:22,  1.21it/s]\u001b[A\nEpoch 1:   2%|▏         | 2/100 [00:01<00:52,  1.88it/s]\u001b[A\nEpoch 1:   3%|▎         | 3/100 [00:01<00:44,  2.16it/s]\u001b[A\nEpoch 1:   4%|▍         | 4/100 [00:01<00:43,  2.18it/s]\u001b[A\nEpoch 1:   5%|▌         | 5/100 [00:02<00:37,  2.54it/s]\u001b[A\nEpoch 1:   6%|▌         | 6/100 [00:02<00:32,  2.90it/s]\u001b[A\nEpoch 1:   7%|▋         | 7/100 [00:02<00:29,  3.12it/s]\u001b[A\nEpoch 1:   8%|▊         | 8/100 [00:03<00:29,  3.11it/s]\u001b[A\nEpoch 1:   9%|▉         | 9/100 [00:03<00:27,  3.34it/s]\u001b[A\nEpoch 1:  10%|█         | 10/100 [00:03<00:25,  3.51it/s]\u001b[A\nEpoch 1:  11%|█         | 11/100 [00:03<00:24,  3.62it/s]\u001b[A\nEpoch 1:  12%|█▏        | 12/100 [00:04<00:25,  3.52it/s]\u001b[A\nEpoch 1:  13%|█▎        | 13/100 [00:04<00:23,  3.64it/s]\u001b[A\nEpoch 1:  14%|█▍        | 14/100 [00:04<00:23,  3.70it/s]\u001b[A\nEpoch 1:  15%|█▌        | 15/100 [00:04<00:22,  3.72it/s]\u001b[A\nEpoch 1:  16%|█▌        | 16/100 [00:05<00:23,  3.58it/s]\u001b[A\nEpoch 1:  17%|█▋        | 17/100 [00:05<00:23,  3.51it/s]\u001b[A\nEpoch 1:  18%|█▊        | 18/100 [00:05<00:24,  3.36it/s]\u001b[A\nEpoch 1:  19%|█▉        | 19/100 [00:06<00:23,  3.38it/s]\u001b[A\nEpoch 1:  20%|██        | 20/100 [00:06<00:24,  3.26it/s]\u001b[A\nEpoch 1:  21%|██        | 21/100 [00:06<00:23,  3.31it/s]\u001b[A\nEpoch 1:  22%|██▏       | 22/100 [00:07<00:22,  3.41it/s]\u001b[A\nEpoch 1:  23%|██▎       | 23/100 [00:07<00:21,  3.53it/s]\u001b[A\nEpoch 1:  24%|██▍       | 24/100 [00:07<00:21,  3.46it/s]\u001b[A\nEpoch 1:  25%|██▌       | 25/100 [00:07<00:21,  3.56it/s]\u001b[A\nEpoch 1:  26%|██▌       | 26/100 [00:08<00:20,  3.63it/s]\u001b[A\nEpoch 1:  27%|██▋       | 27/100 [00:08<00:19,  3.66it/s]\u001b[A\nEpoch 1:  28%|██▊       | 28/100 [00:08<00:20,  3.49it/s]\u001b[A\nEpoch 1:  29%|██▉       | 29/100 [00:08<00:19,  3.59it/s]\u001b[A\nEpoch 1:  30%|███       | 30/100 [00:09<00:19,  3.63it/s]\u001b[A\nEpoch 1:  31%|███       | 31/100 [00:09<00:18,  3.68it/s]\u001b[A\nEpoch 1:  32%|███▏      | 32/100 [00:09<00:19,  3.55it/s]\u001b[A\nEpoch 1:  33%|███▎      | 33/100 [00:10<00:19,  3.52it/s]\u001b[A\nEpoch 1:  34%|███▍      | 34/100 [00:10<00:20,  3.30it/s]\u001b[A\nEpoch 1:  35%|███▌      | 35/100 [00:10<00:20,  3.10it/s]\u001b[A\nEpoch 1:  36%|███▌      | 36/100 [00:11<00:20,  3.08it/s]\u001b[A\nEpoch 1:  37%|███▋      | 37/100 [00:11<00:19,  3.24it/s]\u001b[A\nEpoch 1:  38%|███▊      | 38/100 [00:11<00:18,  3.36it/s]\u001b[A\nEpoch 1:  39%|███▉      | 39/100 [00:11<00:17,  3.48it/s]\u001b[A\nEpoch 1:  40%|████      | 40/100 [00:12<00:17,  3.40it/s]\u001b[A\nEpoch 1:  41%|████      | 41/100 [00:12<00:16,  3.51it/s]\u001b[A\nEpoch 1:  42%|████▏     | 42/100 [00:12<00:16,  3.51it/s]\u001b[A\nEpoch 1:  43%|████▎     | 43/100 [00:13<00:16,  3.48it/s]\u001b[A\nEpoch 1:  44%|████▍     | 44/100 [00:13<00:20,  2.71it/s]\u001b[A\nEpoch 1:  45%|████▌     | 45/100 [00:13<00:18,  2.90it/s]\u001b[A\nEpoch 1:  46%|████▌     | 46/100 [00:14<00:18,  2.88it/s]\u001b[A\nEpoch 1:  47%|████▋     | 47/100 [00:14<00:18,  2.89it/s]\u001b[A\nEpoch 1:  48%|████▊     | 48/100 [00:14<00:17,  3.00it/s]\u001b[A\nEpoch 1:  49%|████▉     | 49/100 [00:15<00:15,  3.25it/s]\u001b[A\nEpoch 1:  50%|█████     | 50/100 [00:15<00:14,  3.40it/s]\u001b[A\nEpoch 1:  51%|█████     | 51/100 [00:15<00:13,  3.52it/s]\u001b[A\nEpoch 1:  52%|█████▏    | 52/100 [00:16<00:14,  3.43it/s]\u001b[A\nEpoch 1:  53%|█████▎    | 53/100 [00:16<00:13,  3.48it/s]\u001b[A\nEpoch 1:  54%|█████▍    | 54/100 [00:16<00:13,  3.53it/s]\u001b[A\nEpoch 1:  55%|█████▌    | 55/100 [00:16<00:12,  3.60it/s]\u001b[A\nEpoch 1:  56%|█████▌    | 56/100 [00:17<00:12,  3.48it/s]\u001b[A\nEpoch 1:  57%|█████▋    | 57/100 [00:17<00:11,  3.60it/s]\u001b[A\nEpoch 1:  58%|█████▊    | 58/100 [00:17<00:11,  3.64it/s]\u001b[A\nEpoch 1:  59%|█████▉    | 59/100 [00:17<00:11,  3.65it/s]\u001b[A\nEpoch 1:  60%|██████    | 60/100 [00:18<00:11,  3.51it/s]\u001b[A\nEpoch 1:  61%|██████    | 61/100 [00:18<00:10,  3.59it/s]\u001b[A\nEpoch 1:  62%|██████▏   | 62/100 [00:18<00:10,  3.67it/s]\u001b[A\nEpoch 1:  63%|██████▎   | 63/100 [00:19<00:09,  3.75it/s]\u001b[A\nEpoch 1:  64%|██████▍   | 64/100 [00:19<00:09,  3.61it/s]\u001b[A\nEpoch 1:  65%|██████▌   | 65/100 [00:19<00:09,  3.72it/s]\u001b[A\nEpoch 1:  66%|██████▌   | 66/100 [00:19<00:08,  3.79it/s]\u001b[A\nEpoch 1:  67%|██████▋   | 67/100 [00:20<00:08,  3.81it/s]\u001b[A\nEpoch 1:  68%|██████▊   | 68/100 [00:20<00:08,  3.65it/s]\u001b[A\nEpoch 1:  69%|██████▉   | 69/100 [00:20<00:08,  3.74it/s]\u001b[A\nEpoch 1:  70%|███████   | 70/100 [00:20<00:07,  3.80it/s]\u001b[A\nEpoch 1:  71%|███████   | 71/100 [00:21<00:07,  3.83it/s]\u001b[A\nEpoch 1:  72%|███████▏  | 72/100 [00:21<00:07,  3.65it/s]\u001b[A\nEpoch 1:  73%|███████▎  | 73/100 [00:21<00:07,  3.74it/s]\u001b[A\nEpoch 1:  74%|███████▍  | 74/100 [00:21<00:06,  3.78it/s]\u001b[A\nEpoch 1:  75%|███████▌  | 75/100 [00:22<00:06,  3.79it/s]\u001b[A\nEpoch 1:  76%|███████▌  | 76/100 [00:22<00:06,  3.61it/s]\u001b[A\nEpoch 1:  77%|███████▋  | 77/100 [00:22<00:06,  3.68it/s]\u001b[A\nEpoch 1:  78%|███████▊  | 78/100 [00:23<00:06,  3.64it/s]\u001b[A\nEpoch 1:  79%|███████▉  | 79/100 [00:23<00:05,  3.71it/s]\u001b[A\nEpoch 1:  80%|████████  | 80/100 [00:23<00:05,  3.58it/s]\u001b[A\nEpoch 1:  81%|████████  | 81/100 [00:23<00:05,  3.70it/s]\u001b[A\nEpoch 1:  82%|████████▏ | 82/100 [00:24<00:04,  3.76it/s]\u001b[A\nEpoch 1:  83%|████████▎ | 83/100 [00:24<00:04,  3.81it/s]\u001b[A\nEpoch 1:  84%|████████▍ | 84/100 [00:24<00:04,  3.65it/s]\u001b[A\nEpoch 1:  85%|████████▌ | 85/100 [00:24<00:04,  3.75it/s]\u001b[A\nEpoch 1:  86%|████████▌ | 86/100 [00:25<00:03,  3.81it/s]\u001b[A\nEpoch 1:  87%|████████▋ | 87/100 [00:25<00:03,  3.84it/s]\u001b[A\nEpoch 1:  88%|████████▊ | 88/100 [00:25<00:03,  3.68it/s]\u001b[A\nEpoch 1:  89%|████████▉ | 89/100 [00:26<00:02,  3.78it/s]\u001b[A\nEpoch 1:  90%|█████████ | 90/100 [00:26<00:02,  3.83it/s]\u001b[A\nEpoch 1:  91%|█████████ | 91/100 [00:26<00:02,  3.88it/s]\u001b[A\nEpoch 1:  92%|█████████▏| 92/100 [00:26<00:02,  3.71it/s]\u001b[A\nEpoch 1:  93%|█████████▎| 93/100 [00:27<00:01,  3.79it/s]\u001b[A\nEpoch 1:  94%|█████████▍| 94/100 [00:27<00:01,  3.82it/s]\u001b[A\nEpoch 1:  95%|█████████▌| 95/100 [00:27<00:01,  3.84it/s]\u001b[A\nEpoch 1:  96%|█████████▌| 96/100 [00:27<00:01,  3.67it/s]\u001b[A\nEpoch 1:  97%|█████████▋| 97/100 [00:28<00:00,  3.78it/s]\u001b[A\nEpoch 1:  98%|█████████▊| 98/100 [00:28<00:00,  3.81it/s]\u001b[A\nEpoch 1:  99%|█████████▉| 99/100 [00:28<00:00,  3.83it/s]\u001b[A\nEpoch 1: 100%|██████████| 100/100 [00:28<00:00,  3.45it/s]\u001b[A\nEpoch 2: 100%|██████████| 100/100 [00:26<00:00,  3.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset-Tuned MolT5 Recommendation: The molecule is an organosulfur heterocyclic compound and an oxacycle that is 1,2,3,4-tetrahydrocyclopenta[b]pyran substituted by a methyl group at position 1 and (1-methylpiperidin-4-yl)methyl groups at positions 4 and 6 respectively. It is metabolite of the proinsecticide tralomethrin.it. it has  cis-Golgi ArfGEF83-E2 (xenobiotic-transporting ATPase) inhibitor.  It has\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.1 </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Generate Morgan fingerprint for query and create graph data\ndef get_morgan_fingerprint_graph(smiles, radius=2, nBits=2048):\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return None\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits)\n        node_feat = torch.FloatTensor(list(fp)).unsqueeze(0)  # Convert to tensor with batch dim\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)  # Simple graph structure\n        return Data(x=node_feat, edge_index=edge_index)\n    except:\n        return None\n\n# Similarity search using HNSW with GIN embeddings\ndef search_similar_compounds(smiles, gin_model, index, k=5):\n    graph_data = get_morgan_fingerprint_graph(smiles)\n    if graph_data is None:\n        return None\n    graph_data = graph_data.to(device)\n    with torch.no_grad():\n        embedding = gin_model(graph_data.x, graph_data.edge_index, torch.zeros(1, dtype=torch.long).to(device))\n        print(f\"Embedding shape before reshape: {embedding.shape}\")  # Debug shape\n        # Reshape to 2D and convert to numpy\n        embedding = embedding.squeeze().cpu().numpy()  # Remove batch dim\n        query = embedding[np.newaxis, :]  # Add batch dimension for FAISS\n        print(f\"Query shape after reshape: {query.shape}\")  # Debug shape\n        if query.shape[1] != index.d:\n            raise ValueError(f\"Query dimension ({query.shape[1]}) does not match index dimension ({index.d})\")\n        distances, indices = index.search(query, k)  # Search with reshaped query\n    return df.iloc[indices[0]]['standard_smiles'].values\n\n# Fine-tune LLM with HNSW-derived compounds\ndef fine_tune_with_hnsw(model, tokenizer, smiles_list, similar_smiles, epochs=1, batch_size=1):\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n    scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n    # Determine model type using config\n    config = model.config\n    print(f\"Model config class: {config.__class__.__name__}\")  # Debug model type\n    if isinstance(config, PretrainedConfig) and hasattr(config, 'model_type'):\n        model_type = config.model_type\n        if model_type in ['biogpt', 'gpt2', 'llama']:  # Causal LM examples\n            model_type = 'causal'\n        elif model_type in ['t5', 'molt5']:  # Seq2Seq examples\n            model_type = 'seq2seq'\n        elif model_type in ['bert', 'roberta', 'chemberta']:  # Classification examples\n            model_type = 'classification'\n        else:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n    else:\n        raise ValueError(\"Unable to determine model type from configuration\")\n\n    # Create training examples combining dataset and HNSW similar compounds\n    templates = [\n        \"This compound, enhanced by similar structures, may exhibit anti-inflammatory properties.\",\n        \"A novel structure for drug development with possible antimicrobial effects based on similar compounds.\",\n        \"This chemical, informed by similar molecules, could be a new lead for cancer therapy research.\"\n    ]\n    train_data = [f\"{smiles}\\t{templates[i % len(templates)]}\" for i, smiles in enumerate(similar_smiles)]\n\n    for epoch in range(epochs):\n        np.random.shuffle(train_data)  # Shuffle to improve learning\n        optimizer.zero_grad()\n        for i in tqdm(range(0, len(train_data), batch_size), desc=f\"HNSW Tuning Epoch {epoch+1}\"):\n            batch = train_data[i:i+batch_size]\n            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n            \n            with torch.cuda.amp.autocast():\n                if model_type == 'causal':\n                    outputs = model(**inputs, labels=inputs['input_ids'])\n                elif model_type == 'seq2seq':\n                    decoder_input_ids = inputs['input_ids'].clone()\n                    decoder_input_ids[:, 1:] = decoder_input_ids[:, :-1].clone()  # Shift for teacher forcing\n                    decoder_input_ids[:, 0] = tokenizer.pad_token_id  # Start with pad token\n                    outputs = model(**inputs, decoder_input_ids=decoder_input_ids, labels=inputs['input_ids'])\n                elif model_type == 'classification':\n                    labels = torch.zeros(len(batch), dtype=torch.long).to(device)  # Dummy labels\n                    outputs = model(**inputs, labels=labels)\n                loss = outputs.loss  # No accumulation for now, adjust if needed\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n    model.eval()\n\n# Generate recommendation using HNSW-tuned LLM\ndef generate_hnsw_tuned_recommendation(model, tokenizer):\n    prompt = f\"Generate a novel chemical compound for drug development. Provide a SMILES string and its potential therapeutic application.\"\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n    \n    if hasattr(model.config, 'model_type') and model.config.model_type in ['biogpt', 'gpt2', 'llama']:\n        outputs = model.generate(\n            **inputs,\n            max_length=200,\n            num_return_sequences=1,\n            do_sample=True,\n            top_k=40,\n            top_p=0.92,\n            temperature=0.8,\n            no_repeat_ngram_size=2\n        )\n    elif hasattr(model.config, 'model_type') and model.config.model_type in ['t5', 'molt5']:\n        outputs = model.generate(\n            **inputs,\n            max_length=200,\n            num_return_sequences=1,\n            do_sample=True,\n            top_k=40,\n            top_p=0.92,\n            temperature=0.8,\n            no_repeat_ngram_size=2,\n            decoder_start_token_id=tokenizer.pad_token_id\n        )\n    elif hasattr(model.config, 'model_type') and model.config.model_type in ['bert', 'roberta', 'chemberta']:\n        return \"ChemBERTa is a classification model and cannot generate recommendations directly.\"\n    else:\n        raise ValueError(f\"Unsupported model type for generation: {model.config.model_type if hasattr(model.config, 'model_type') else 'Unknown'}\")\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n\nprint(\"HNSW TUNING COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:23:29.833279Z","iopub.execute_input":"2025-06-08T17:23:29.833576Z","iopub.status.idle":"2025-06-08T17:23:29.849938Z","shell.execute_reply.started":"2025-06-08T17:23:29.833551Z","shell.execute_reply":"2025-06-08T17:23:29.849273Z"}},"outputs":[{"name":"stdout","text":"HNSW TUNING COMPLETE\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.4 </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# HNSW-Tuned recommendation\nquery_smiles = df['standard_smiles'].iloc[0]  # Example query\nsimilar_smiles = search_similar_compounds(query_smiles, gin_model, index)\nhnsw_tuned_rec = None\nif similar_smiles is not None:\n    print(\"Similar Compounds:\")\n    for smi in similar_smiles:\n        print(smi)\n    # Re-tune LLM with HNSW-derived compounds\n    fine_tune_with_hnsw(llm_configs[active_llm]['model'], llm_configs[active_llm]['tokenizer'], df['standard_smiles'].tolist()[:100], similar_smiles)\n    hnsw_tuned_rec = generate_hnsw_tuned_recommendation(\n        llm_configs[active_llm]['model'], llm_configs[active_llm]['tokenizer']\n    )\n    print(f\"HNSW-Tuned {active_llm} Recommendation: {hnsw_tuned_rec}\")\nelse:\n    print(\"HNSW search failed due to invalid query SMILES or index mismatch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:23:29.850698Z","iopub.execute_input":"2025-06-08T17:23:29.850940Z","iopub.status.idle":"2025-06-08T17:23:34.404949Z","shell.execute_reply.started":"2025-06-08T17:23:29.850923Z","shell.execute_reply":"2025-06-08T17:23:34.404218Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/877543561.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n","output_type":"stream"},{"name":"stdout","text":"Embedding shape before reshape: torch.Size([1, 256])\nQuery shape after reshape: (1, 256)\nSimilar Compounds:\nN#CC1CC(NCCO)CCC1n1cc(C(N)=O)c(Nc2ccc(S(=O)(=O)C(F)(F)F)cc2)n1\nNc1ccc2c(c1)Cc1ccccc1-2\nO=C1C(=C2Nc3ccccc3C2=O)Nc2ccccc21\nNc1ncnc2ncn(C(c3ccccc3)c3ccccc3)c12\nCOc1ccc(OC)c(Cc2cnc(N)nc2N)c1\nModel config class: T5Config\n","output_type":"stream"},{"name":"stderr","text":"HNSW Tuning Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_35/877543561.py:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nHNSW Tuning Epoch 1: 100%|██████████| 5/5 [00:01<00:00,  3.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"HNSW-Tuned MolT5 Recommendation: The molecule is a member of the class of dithiocarbamic acids that is 1,3-dithiol substituted at position 2 by  A methyl group and at positions 4 and 5 by chlorine and morpholin-4-yl groups respectively. It is an organosulfur heterocyclic compound anda dihydrocarbyl phosphate.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Saving Recommendations </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/recommendations.txt', 'w') as f:\n    f.write(f\"Dataset-Tuned {active_llm} Recommendation: {dataset_tuned_rec}\\n\")\n    f.write(f\"HNSW-Tuned {active_llm} Recommendation: {hnsw_tuned_rec}\\n\")\nprint(\"Recommendations saved to /kaggle/working/recommendations.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:23:34.405688Z","iopub.execute_input":"2025-06-08T17:23:34.405923Z","iopub.status.idle":"2025-06-08T17:23:34.410768Z","shell.execute_reply.started":"2025-06-08T17:23:34.405904Z","shell.execute_reply":"2025-06-08T17:23:34.410188Z"}},"outputs":[{"name":"stdout","text":"Recommendations saved to /kaggle/working/recommendations.txt\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
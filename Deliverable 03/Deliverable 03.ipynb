{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11487321,"sourceType":"datasetVersion","datasetId":7200232},{"sourceId":11944874,"sourceType":"datasetVersion","datasetId":7509207}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div>\n    <h1 align=\"center\"><font color=\"blue\"> DELIVERABLE 3 </font></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Downloading Libraries </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"pip install rdkit-pypi torch_geometric faiss-cpu sacremoses langchain langchain-community langchain-openai --quiet","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab":{"base_uri":"https://localhost:8080/"},"id":"dXsSaIFHPEAc","outputId":"5667d7dd-a341-4c9e-a923-38a5ae4af8c8","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:29:45.547331Z","iopub.execute_input":"2025-06-09T08:29:45.547926Z","iopub.status.idle":"2025-06-09T08:30:00.923171Z","shell.execute_reply.started":"2025-06-09T08:29:45.547904Z","shell.execute_reply":"2025-06-09T08:30:00.922280Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# rdkit-pypi: Helps me work with chemical structures and SMILES strings for molecules.\n# torch_geometric: Allows me to build graph neural networks (GNNs) for processing molecular data.\n# faiss-cpu: Used for fast similarity searches with embeddings, like finding similar compounds.\n# sacremoses: Likely needed for text processing, possibly for the language model part.\n# bitsandbytes: Helps with memory-efficient model training, especially for large language models.\n                                             \nprint(\"---------- ALL LIBRARIES HAVE BEEN DOWNLOADED ----------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:00.925046Z","iopub.execute_input":"2025-06-09T08:30:00.925327Z","iopub.status.idle":"2025-06-09T08:30:00.930007Z","shell.execute_reply.started":"2025-06-09T08:30:00.925304Z","shell.execute_reply":"2025-06-09T08:30:00.929106Z"}},"outputs":[{"name":"stdout","text":"---------- ALL LIBRARIES HAVE BEEN DOWNLOADED ----------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Importing Libraries </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINConv, global_add_pool\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.loader import DataLoader\nimport numpy as np\nimport pandas as pd\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, DataStructs\n\nimport faiss\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, PretrainedConfig\nfrom rdkit.Chem import Descriptors\nfrom tqdm import tqdm\nimport gc\nimport os\nimport ast\nimport re\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.amp import GradScaler, autocast\nfrom sklearn.model_selection import train_test_split\nfrom rdkit import RDLogger\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nprint(\"---------- ALL LIBRARIES HAVE BEEN IMPORTED ----------\")\n\n# torch, torch.nn, and torch.nn.functional: For building and training neural networks, like my GNN model.\n# torch_geometric modules (GINConv, global_add_pool, Data, Batch, DataLoader): Help me create and process graph-based data for molecules.\n# rdkit modules (Chem, AllChem, DataStructs, Descriptors): lets me work with chemical structures, generate fingerprints, and calculate properties like logP.\n# faiss: For efficient similarity searches using embeddings.\n# transformers modules (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig): For loading and using large language models (LLMs) like BioMistral.\n# tqdm: Adds progress bars to loops, so I can see how long processes take.\n# gc: Helps manage memory by cleaning up unused objects.\n# ast and re: For parsing strings and extracting information from text, like LLM outputs.\n# torch.cuda.amp (GradScaler, autocast): Optimizes training on GPUs to save memory and speed up computations.\n# bitsandbytes: Reduces memory usage for LLMs.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:00.930963Z","iopub.execute_input":"2025-06-09T08:30:00.931185Z","iopub.status.idle":"2025-06-09T08:30:14.042744Z","shell.execute_reply.started":"2025-06-09T08:30:00.931159Z","shell.execute_reply":"2025-06-09T08:30:14.041942Z"}},"outputs":[{"name":"stdout","text":"---------- ALL LIBRARIES HAVE BEEN IMPORTED ----------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"<div>\n    <h2 align=\"center\"><font color=\"purple\"> Deliverable 1 Code </font></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Set device for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:14.044915Z","iopub.execute_input":"2025-06-09T08:30:14.045364Z","iopub.status.idle":"2025-06-09T08:30:14.050226Z","shell.execute_reply.started":"2025-06-09T08:30:14.045345Z","shell.execute_reply":"2025-06-09T08:30:14.049137Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 01: Data Loading and Preprocessing </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/smiles/SMILES_Big_Data_Set.csv')\nprint(\"Dataset columns:\", df.columns.tolist())\n\n# Standardizing SMILES strings to ensure consistency and track invalid ones.\ninvalid_smiles_count = 0\ndef standardize_smiles(smiles):\n    global invalid_smiles_count\n    try:\n        mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object.\n        if mol is None:\n            invalid_smiles_count += 1 \n            return None\n        return Chem.MolToSmiles(mol, isomericSmiles=True)  # Convert back to standardized SMILES.\n    except:\n        invalid_smiles_count += 1  # Increment counter if conversion fails.\n        return None\n\ndf['standard_smiles'] = df['SMILES'].apply(standardize_smiles) \ndf = df.dropna(subset=['standard_smiles']).drop_duplicates(subset=['standard_smiles'])\nprint(f\"Removed {invalid_smiles_count} invalid SMILES strings.\")\n\n\ndf['pIC50'] = pd.to_numeric(df['pIC50'], errors='coerce') \ndf['num_atoms'] = pd.to_numeric(df['num_atoms'], errors='coerce')  \ndf['logP'] = pd.to_numeric(df['logP'], errors='coerce') \ndf = df.dropna() \n\n# Creating a column of RDKit molecule objects for later use, like generating fingerprints.\ndf['mol'] = df['standard_smiles'].apply(Chem.MolFromSmiles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:14.051182Z","iopub.execute_input":"2025-06-09T08:30:14.051992Z","iopub.status.idle":"2025-06-09T08:30:19.120180Z","shell.execute_reply.started":"2025-06-09T08:30:14.051954Z","shell.execute_reply":"2025-06-09T08:30:19.119572Z"}},"outputs":[{"name":"stdout","text":"Dataset columns: ['SMILES', 'pIC50', 'mol', 'num_atoms', 'logP']\nRemoved 0 invalid SMILES strings.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 02: Generating Fingerprints (Morgan Fingerprints) </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Creating Morgan fingerprints to represent molecular structures numerically for GNN input.\ndef generate_morgan_fingerprint(mol, radius=2, n_bits=2048):\n    if mol is None:\n        return None\n    try:\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)  # Generate 2048-bit Morgan fingerprint with radius 2.\n        arr = np.zeros((n_bits,), dtype=np.float32)\n        DataStructs.ConvertToNumpyArray(fp, arr)  # Convert fingerprint to NumPy array of 0s and 1s.\n        return arr\n    except:\n        return None\n\ndf['morgan_fp'] = df['mol'].apply(generate_morgan_fingerprint)  \ndf = df[df['morgan_fp'].notnull()]  # Remove rows where fingerprint generation failed.\nfp_matrix = np.stack(df['morgan_fp'].values)  # Stack all fingerprints into a single NumPy array for GNN training.\nprint(f\"Fingerprint matrix shape: {fp_matrix.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-06-09T08:30:19.121017Z","iopub.execute_input":"2025-06-09T08:30:19.121356Z","iopub.status.idle":"2025-06-09T08:30:20.916134Z","shell.execute_reply.started":"2025-06-09T08:30:19.121337Z","shell.execute_reply":"2025-06-09T08:30:20.915425Z"},"id":"iHGFFs4kPEAd","outputId":"152d69c8-405a-4e3d-fb0e-36047bb5b921","trusted":true},"outputs":[{"name":"stdout","text":"Fingerprint matrix shape: (14823, 2048)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 03: GNN for Fingerprint Embedding (GIN) </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Defining a Graph Neural Network (GNN) to create compact embeddings from Morgan fingerprints.\nclass FingerprintGNN(nn.Module):\n    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=256):\n        super().__init__()\n        self.fp_to_node = nn.Linear(input_dim, hidden_dim)  # Reduce 2048-bit fingerprint to 512 dimensions.\n        self.conv1 = GINConv(nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),  # First linear layer for graph convolution.\n            nn.ReLU(),  # Activation\n            nn.Linear(hidden_dim, hidden_dim)  # Second linear layer for feature transformation.\n        ))\n        self.conv2 = GINConv(nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),  # Second graph convolution layer.\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        ))\n        self.lin = nn.Linear(hidden_dim, output_dim)  # Final layer to output 256-dimensional embedding.\n\n    def forward(self, x, edge_index, batch):\n        x = self.fp_to_node(x)  # Transform input fingerprint to hidden dimension.\n        x = self.conv1(x, edge_index).relu() \n        x = self.conv2(x, edge_index) \n        pooled = global_add_pool(x, batch)  # Aggregate node features into a single embedding per graph.\n        return self.lin(pooled)  \n\ndata_list = []\nfor fp in df['morgan_fp']:\n    node_feat = torch.FloatTensor(fp).unsqueeze(0)  # Convert fingerprint to tensor and add batch dimension.\n    edge_index = torch.tensor([[0], [0]], dtype=torch.long) \n    data = Data(x=node_feat, edge_index=edge_index) \n    data_list.append(data)\n\nbatch_size = 128  # Set batch size for efficient training.\nloader = DataLoader(data_list, batch_size=batch_size, shuffle=False)  # Create DataLoader for batching graphs.\n\n# Training the GNN model using an autoencoder-like loss.\ngin_model = FingerprintGNN().to(device)  \noptimizer = torch.optim.Adam(gin_model.parameters(), lr=0.001)  # Set up Adam optimizer.\ntarget_projection = nn.Linear(2048, 256).to(device)  # Linear layer to project fingerprints to 256 dimensions for loss calculation.\n\n# Ensure=ing target_projection parameters are optimized along with GNN.\ncombined_params = list(gin_model.parameters()) + list(target_projection.parameters())\noptimizer = torch.optim.Adam(combined_params, lr=0.001) \n\nepochs = 10\n\nprint(\"\\nTraining GIN model...\")\nfor epoch in range(epochs):\n    gin_model.train()  \n    target_projection.train() \n    total_loss = 0\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = gin_model(batch.x, batch.edge_index, batch.batch)  # Get GNN embeddings.\n        target = target_projection(batch.x) \n        loss = F.mse_loss(out, target)  # Calculate MSE loss between GNN and projected embeddings.\n        loss.backward()\n        optimizer.step()  # Update model weights.\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader)}\")\n\n# Generating embeddings for all fingerprints using the trained GNN.\nprint(\"\\nGenerating GNN embeddings...\")\ngin_model.eval()  \ntarget_projection.eval() \nembeddings = []\nwith torch.no_grad():  # Disable gradient tracking to save memory.\n    for batch in loader:\n        batch = batch.to(device) \n        emb = gin_model(batch.x, batch.edge_index, batch.batch)  # Generate embeddings.\n        embeddings.append(emb.cpu().numpy()) \nembedding_matrix = np.vstack(embeddings) \nprint(f\"Embedding matrix shape: {embedding_matrix.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:20.916772Z","iopub.execute_input":"2025-06-09T08:30:20.916960Z","iopub.status.idle":"2025-06-09T08:30:30.494654Z","shell.execute_reply.started":"2025-06-09T08:30:20.916946Z","shell.execute_reply":"2025-06-09T08:30:30.493959Z"}},"outputs":[{"name":"stdout","text":"\nTraining GIN model...\nEpoch 1, Loss: 0.001420613777209555\nEpoch 2, Loss: 0.00047226974957397785\nEpoch 3, Loss: 0.0003054972710680008\nEpoch 4, Loss: 0.0003364992929885469\nEpoch 5, Loss: 0.0005024994230740864\nEpoch 6, Loss: 0.0010846421520972367\nEpoch 7, Loss: 0.003103570847527188\nEpoch 8, Loss: 0.001280652909591826\nEpoch 9, Loss: 0.0014062831012716774\nEpoch 10, Loss: 0.0006510658833952941\n\nGenerating GNN embeddings...\nEmbedding matrix shape: (14823, 256)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Saving preprocessed data, embeddings, trained model </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"# Saving my processed data and trained GNN model for later use.\ndf['gnn_embedding'] = embedding_matrix.tolist() \ndf.to_csv('preprocessed_data_with_embeddings.csv', index=False) \n\n# Saving the GNN model's weights to a file.\ntorch.save(gin_model.state_dict(), \"gin_model.pth\") \n\nprint(\"Data Saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:30.495345Z","iopub.execute_input":"2025-06-09T08:30:30.495530Z","iopub.status.idle":"2025-06-09T08:30:36.733347Z","shell.execute_reply.started":"2025-06-09T08:30:30.495515Z","shell.execute_reply":"2025-06-09T08:30:36.732629Z"}},"outputs":[{"name":"stdout","text":"Data Saved!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Checking if required columns exist in df </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"# Checking if my DataFrame has the necessary columns for later steps.\nif 'gnn_embedding' not in df.columns or 'standard_smiles' not in df.columns:\n    raise ValueError(\"Required columns 'gnn_embedding' or 'standard_smiles' not found in DataFrame.\")\nelse:\n    print(\"Required Columns Exist!\")\n\n# Resetting the DataFrame index to align with the embedding matrix.\ndf = df.reset_index(drop=True)  # Ensure row indices match embedding matrix to avoid mismatches.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:36.734009Z","iopub.execute_input":"2025-06-09T08:30:36.734195Z","iopub.status.idle":"2025-06-09T08:30:36.742253Z","shell.execute_reply.started":"2025-06-09T08:30:36.734180Z","shell.execute_reply":"2025-06-09T08:30:36.741614Z"}},"outputs":[{"name":"stdout","text":"Required Columns Exist!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 04: HNSW Index for GNN Embeddings </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Converting GNN embeddings to a NumPy array for Faiss.\nembedding_matrix = np.stack(df['gnn_embedding'].values).astype(np.float32)  \nembedding_dim = embedding_matrix.shape[1] \n\nindex = faiss.IndexHNSWFlat(embedding_dim, 32)  # Create HNSW index with M=32 (graph degree).\nindex.hnsw.efConstruction = 200  # Set construction parameter for better index quality.\nindex.hnsw.efSearch = 100  # Set search parameter for better accuracy.\nfaiss.normalize_L2(embedding_matrix)  # Normalize embeddings for cosine similarity.\n\nindex.add(embedding_matrix)  # Index all embeddings for similarity searches.\nprint(f\"Indexed {embedding_matrix.shape[0]} compounds.\")\n\n# Saving the index to a file for later use.\nfaiss.write_index(index, \"gnn_hnsw_index.faiss\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:36.744295Z","iopub.execute_input":"2025-06-09T08:30:36.744479Z","iopub.status.idle":"2025-06-09T08:30:37.848338Z","shell.execute_reply.started":"2025-06-09T08:30:36.744466Z","shell.execute_reply":"2025-06-09T08:30:37.847579Z"}},"outputs":[{"name":"stdout","text":"Indexed 14823 compounds.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> STEP 05: HNSW Search Function </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Defining a function to find compounds similar to a query fingerprint using the HNSW index.\ndef search_similar_compounds(query_fp, gin_model, index, top_k=5, device='cpu'):\n    \"\"\"\n    Search for compounds similar to the query fingerprint using HNSW index.\n    \"\"\"\n    try:\n        # Setting up the GNN model to generate embeddings for the query.\n        gin_model.eval() \n        gin_model.to(device) \n\n        query_fp = np.array(query_fp, dtype=np.float32)  \n        node_feat = torch.FloatTensor(query_fp).unsqueeze(0).to(device) \n        edge_index = torch.tensor([[0], [0]], dtype=torch.long).to(device)  # Create self-loop for single-node graph.\n        data = Data(x=node_feat, edge_index=edge_index)  # Wrap in Data object.\n        batch = torch.zeros(1, dtype=torch.long).to(device)  # Batch tensor for single graph.\n\n        with torch.no_grad(): \n            query_embedding = gin_model(data.x, data.edge_index, batch).cpu().numpy()  # Get 256-dimensional embedding.\n        \n        query_embedding = query_embedding.astype(np.float32) \n        faiss.normalize_L2(query_embedding)\n\n        # Searching for the top_k most similar compounds.\n        _, indices = index.search(query_embedding, top_k)  \n\n        # Retrieving the SMILES strings of similar compounds.\n        similar_smiles = df.iloc[indices[0]]['standard_smiles'].values.tolist() \n        return similar_smiles\n    \n    except Exception as e:\n        print(f\"Error during similarity search: {e}\")\n        return []  \n\nprint(\"Similar Compound Search Function made!\")","metadata":{"execution":{"iopub.status.busy":"2025-06-09T08:30:37.849002Z","iopub.execute_input":"2025-06-09T08:30:37.849183Z","iopub.status.idle":"2025-06-09T08:30:37.856580Z","shell.execute_reply.started":"2025-06-09T08:30:37.849170Z","shell.execute_reply":"2025-06-09T08:30:37.855865Z"},"id":"gKO3krgLPEAe","outputId":"ec6c4291-c092-40d0-99eb-7331b4151e2f","trusted":true},"outputs":[{"name":"stdout","text":"Similar Compound Search Function made!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Example Search Using HNSW </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"print(\"\\nSearching for similar compounds...\")\n\n# Testing the similarity search with a sample SMILES string.\nquery_smiles = \"NS(=O)(=O)N1CCC(NC(=O)c2cnn3ccc(N4CCCC4c4cc(F)ccc4F)nc23)CC1\"\nquery_mol = Chem.MolFromSmiles(query_smiles)  # Convert SMILES to RDKit molecule.\nif query_mol is None:\n    print(\"Error: Invalid query SMILES string.\")\nelse:\n    query_fp = generate_morgan_fingerprint(query_mol)  # Generate Morgan fingerprint for query.\n    if query_fp is None:\n        print(\"Error: Failed to generate fingerprint for query molecule.\")\n    else:\n        # Using the search function to find similar compounds.\n        similar_compounds = search_similar_compounds(query_fp, gin_model, index, top_k=5, device=device)  # Find top 5 similar compounds.\n        print(\"\\nTop 5 Similar Compounds:\")\n        for i, smiles in enumerate(similar_compounds, 1):\n            print(f\"{i}. {smiles}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-09T08:30:37.857289Z","iopub.execute_input":"2025-06-09T08:30:37.857491Z","iopub.status.idle":"2025-06-09T08:30:37.893353Z","shell.execute_reply.started":"2025-06-09T08:30:37.857472Z","shell.execute_reply":"2025-06-09T08:30:37.892701Z"},"id":"T39I6YSRPEAf","outputId":"4e5f57a5-38c5-4a56-fa13-0e678033614e","trusted":true},"outputs":[{"name":"stdout","text":"\nSearching for similar compounds...\n\nTop 5 Similar Compounds:\n1. NS(=O)(=O)N1CCC(NC(=O)c2cnn3ccc(N4CCCC4c4cc(F)ccc4F)nc23)CC1\n2. CC(C)N(C=Nc1ccn(C2CCC(CO)O2)c(=O)n1)C(C)C\n3. CCCS(=O)(=O)CC1CC(N(C)c2[nH]cnc3nccc2-3)C1\n4. CS(=O)(=O)c1cnc2ccccc2n1\n5. CCCC1CCCc2ncc(C(=O)OC)c(=O)n21\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"<div>\n    <h2 align=\"center\"><font color=\"purple\"> Deliverable 2 Code </font></h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Suppress Warnings </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"RDLogger.DisableLog('rdApp.*')\n\nprint(\"Suppress command executed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:37.893989Z","iopub.execute_input":"2025-06-09T08:30:37.894303Z","iopub.status.idle":"2025-06-09T08:30:37.900016Z","shell.execute_reply.started":"2025-06-09T08:30:37.894284Z","shell.execute_reply":"2025-06-09T08:30:37.899338Z"}},"outputs":[{"name":"stdout","text":"Suppress command executed!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Validating and Retrieving data from Deliverable 01 </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"# Loading preprocessed data from Deliverable 1\ndf = pd.read_csv('/kaggle/working/preprocessed_data_with_embeddings.csv')\nprint(\"Loaded columns:\", df.columns)\n\n# Validating required columns\nrequired_columns = ['standard_smiles', 'gnn_embedding']\nmissing = [col for col in required_columns if col not in df.columns]\nif missing:\n    raise ValueError(f\"Missing columns: {missing}\")\n\n# Loading FAISS HNSW index\nd = len(df['gnn_embedding'].iloc[0])  # Embedding dimension\nindex = faiss.read_index('/kaggle/working/gnn_hnsw_index.faiss')\nprint(\"HNSW index loaded with\", index.ntotal, \"embeddings\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:37.900797Z","iopub.execute_input":"2025-06-09T08:30:37.901549Z","iopub.status.idle":"2025-06-09T08:30:39.009476Z","shell.execute_reply.started":"2025-06-09T08:30:37.901525Z","shell.execute_reply":"2025-06-09T08:30:39.008601Z"}},"outputs":[{"name":"stdout","text":"Loaded columns: Index(['SMILES', 'pIC50', 'mol', 'num_atoms', 'logP', 'standard_smiles',\n       'morgan_fp', 'gnn_embedding'],\n      dtype='object')\nHNSW index loaded with 14823 embeddings\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Loading Pre-trained GIN Model </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"gin_model = FingerprintGNN().to(device)\ngin_model.load_state_dict(torch.load('/kaggle/working/gin_model.pth'))\ngin_model.eval()\n\nprint(\"Loaded pretrained GIN model successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:39.010372Z","iopub.execute_input":"2025-06-09T08:30:39.010650Z","iopub.status.idle":"2025-06-09T08:30:39.052157Z","shell.execute_reply.started":"2025-06-09T08:30:39.010631Z","shell.execute_reply":"2025-06-09T08:30:39.051563Z"}},"outputs":[{"name":"stdout","text":"Loaded pretrained GIN model successfully.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.2 (Step 01) </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Dictionary to store LLM configurations (only one active at a time)\nllm_configs = {\n    # 'BioGPT': {\n    #     'model_name': 'microsoft/biogpt',\n    #     'tokenizer': AutoTokenizer.from_pretrained('microsoft/biogpt'),\n    #     'model': AutoModelForCausalLM.from_pretrained('microsoft/biogpt').to(device)\n    # },\n    'MolT5': {\n        'model_name': 'laituan245/molt5-large-smiles2caption',\n        'tokenizer': AutoTokenizer.from_pretrained('laituan245/molt5-large-smiles2caption'),\n        'model': AutoModelForSeq2SeqLM.from_pretrained('laituan245/molt5-large-smiles2caption').to(device)\n    },\n    # 'ChemBERTa': {\n    #     'model_name': 'DeepChem/ChemBERTa-77M-MTR',\n    #     'tokenizer': AutoTokenizer.from_pretrained('DeepChem/ChemBERTa-77M-MTR'),\n    #     'model': AutoModelForSequenceClassification.from_pretrained('DeepChem/ChemBERTa-77M-MTR').to(device)\n    # }\n}\nactive_llm = 'MolT5'  # Change to 'BioGPT', 'MolT5', or 'ChemBERTa' to switch models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:30:39.052892Z","iopub.execute_input":"2025-06-09T08:30:39.053072Z","iopub.status.idle":"2025-06-09T08:31:09.057528Z","shell.execute_reply.started":"2025-06-09T08:30:39.053058Z","shell.execute_reply":"2025-06-09T08:31:09.056570Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a348e13dfc4e61b8c0210b83eb5d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3513e61107004315a7bdd97b22ca7ee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e949e6c27f0b4cd8afa55456b583f7fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99e9653bb07b48798be9d2db1896b8a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ba719f54b54213a745aa8dd12eadc1"}},"metadata":{}},{"name":"stderr","text":"2025-06-09 08:30:42.908154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749457843.095892      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749457843.149957      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9cabffc551447ac84ea58c2e33459b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160e4706ef7349628c828410d3ca5802"}},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.2 (Step 02) & R2.3 (Step 01) </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Set environment variable for CUDA memory optimization\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Fine-tune active LLM on SMILES dataset with diverse prompts\ndef fine_tune_llm(model, tokenizer, smiles_list, epochs=2, batch_size=1, accumulation_steps=4):\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n    scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n    # Determine model type using config\n    config = model.config\n    print(f\"Model config class: {config.__class__.__name__}\")  # Debug model type\n    if isinstance(config, PretrainedConfig) and hasattr(config, 'model_type'):\n        model_type = config.model_type\n        if model_type in ['biogpt', 'gpt2', 'llama']:  # Causal LM examples\n            model_type = 'causal'\n        elif model_type in ['t5', 'molt5']:  # Seq2Seq examples\n            model_type = 'seq2seq'\n        elif model_type in ['bert', 'roberta', 'chemberta']:  # Classification examples\n            model_type = 'classification'\n        else:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n    else:\n        raise ValueError(\"Unable to determine model type from configuration\")\n\n    # Create training examples with varied targets\n    templates = [\n        \"This compound, a potential drug candidate, may exhibit anti-inflammatory properties.\",\n        \"A novel structure for drug development with possible antimicrobial effects.\",\n        \"This chemical could be a new lead for cancer therapy research.\"\n    ]\n    train_data = [f\"{smiles}\\t{templates[i % len(templates)]}\" for i, smiles in enumerate(smiles_list)]\n\n    for epoch in range(epochs):\n        np.random.shuffle(train_data)  # Shuffle to improve learning\n        optimizer.zero_grad()\n        for i in tqdm(range(0, len(train_data), batch_size), desc=f\"Epoch {epoch+1}\"):\n            batch = train_data[i:i+batch_size]\n            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n            \n            with torch.cuda.amp.autocast():\n                if model_type == 'causal':\n                    outputs = model(**inputs, labels=inputs['input_ids'])\n                elif model_type == 'seq2seq':\n                    decoder_input_ids = inputs['input_ids'].clone()\n                    decoder_input_ids[:, 1:] = decoder_input_ids[:, :-1].clone()  # Shift for teacher forcing\n                    decoder_input_ids[:, 0] = tokenizer.pad_token_id  # Start with pad token\n                    outputs = model(**inputs, decoder_input_ids=decoder_input_ids, labels=inputs['input_ids'])\n                elif model_type == 'classification':\n                    labels = torch.zeros(len(batch), dtype=torch.long).to(device)  # Dummy labels\n                    outputs = model(**inputs, labels=labels)\n                loss = outputs.loss / accumulation_steps  # Scale loss for accumulation\n\n            scaler.scale(loss).backward()\n            if (i + 1) % accumulation_steps == 0 or i + 1 == len(train_data):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n    model.eval()\n\n# Generate recommendation using dataset-tuned LLM\ndef generate_dataset_tuned_recommendation(model, tokenizer, prompt):\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n    \n    if hasattr(model.config, 'model_type') and model.config.model_type in ['biogpt', 'gpt2', 'llama']:\n        outputs = model.generate(\n            **inputs,\n            max_length=200,\n            num_return_sequences=1,\n            do_sample=True,\n            top_k=40,\n            top_p=0.92,\n            temperature=0.8,\n            no_repeat_ngram_size=2\n        )\n    elif hasattr(model.config, 'model_type') and model.config.model_type in ['t5', 'molt5']:\n        outputs = model.generate(\n            **inputs,\n            max_length=200,\n            num_return_sequences=1,\n            do_sample=True,\n            top_k=40,\n            top_p=0.92,\n            temperature=0.8,\n            no_repeat_ngram_size=2,\n            decoder_start_token_id=tokenizer.pad_token_id\n        )\n    elif hasattr(model.config, 'model_type') and model.config.model_type in ['bert', 'roberta', 'chemberta']:\n        return \"ChemBERTa is a classification model and cannot generate recommendations directly.\"\n    else:\n        raise ValueError(f\"Unsupported model type for generation: {model.config.model_type if hasattr(model.config, 'model_type') else 'Unknown'}\")\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n\nprint(\"DATASET TUNING COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:31:09.058393Z","iopub.execute_input":"2025-06-09T08:31:09.059068Z","iopub.status.idle":"2025-06-09T08:31:09.073565Z","shell.execute_reply.started":"2025-06-09T08:31:09.059047Z","shell.execute_reply":"2025-06-09T08:31:09.072678Z"}},"outputs":[{"name":"stdout","text":"DATASET TUNING COMPLETE\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.4 </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Generate recommendation using Dataset\nsmiles_list = df['standard_smiles'].tolist()[:100]  # Reduced to 100 SMILES for memory\nprint(f\"Tuning on first 100 SMILES: {smiles_list[:5]}... (total {len(smiles_list)})\")  # Debug\nfine_tune_llm(llm_configs[active_llm]['model'], llm_configs[active_llm]['tokenizer'], smiles_list)\nprompt = \"Propose a novel chemical compound for drug development, including a SMILES string and its potential therapeutic application.\"\ndataset_tuned_rec = generate_dataset_tuned_recommendation(\n    llm_configs[active_llm]['model'], llm_configs[active_llm]['tokenizer'], prompt\n)\n\nprint(f\"Dataset-Tuned {active_llm} Recommendation: {dataset_tuned_rec}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:31:09.074388Z","iopub.execute_input":"2025-06-09T08:31:09.074628Z","iopub.status.idle":"2025-06-09T08:32:17.249380Z","shell.execute_reply.started":"2025-06-09T08:31:09.074608Z","shell.execute_reply":"2025-06-09T08:32:17.248689Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3085775951.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n","output_type":"stream"},{"name":"stdout","text":"Tuning on first 100 SMILES: ['O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1', 'O=c1cc(-c2nc(-c3ccc(-c4cn(CCP(=O)(O)O)nn4)cc3)[nH]c2-c2ccc(F)cc2)cc[nH]1', 'NC(=O)c1ccc2c(c1)nc(C1CCC(O)CC1)n2CCCO', 'NCCCn1c(C2CCNCC2)nc2cc(C(N)=O)ccc21', 'CNC(=S)Nc1cccc(-c2cnc3ccccc3n2)c1']... (total 100)\nModel config class: T5Config\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A/tmp/ipykernel_35/3085775951.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n\nEpoch 1:   1%|          | 1/100 [00:00<01:18,  1.26it/s]\u001b[A\nEpoch 1:   2%|▏         | 2/100 [00:01<01:00,  1.62it/s]\u001b[A\nEpoch 1:   3%|▎         | 3/100 [00:01<00:58,  1.66it/s]\u001b[A\nEpoch 1:   4%|▍         | 4/100 [00:02<01:02,  1.55it/s]\u001b[A\nEpoch 1:   5%|▌         | 5/100 [00:03<00:57,  1.66it/s]\u001b[A\nEpoch 1:   6%|▌         | 6/100 [00:03<00:50,  1.86it/s]\u001b[A\nEpoch 1:   7%|▋         | 7/100 [00:03<00:44,  2.08it/s]\u001b[A\nEpoch 1:   8%|▊         | 8/100 [00:04<00:42,  2.16it/s]\u001b[A\nEpoch 1:   9%|▉         | 9/100 [00:04<00:38,  2.38it/s]\u001b[A\nEpoch 1:  10%|█         | 10/100 [00:04<00:36,  2.49it/s]\u001b[A\nEpoch 1:  11%|█         | 11/100 [00:05<00:32,  2.75it/s]\u001b[A\nEpoch 1:  12%|█▏        | 12/100 [00:05<00:30,  2.88it/s]\u001b[A\nEpoch 1:  13%|█▎        | 13/100 [00:05<00:28,  3.09it/s]\u001b[A\nEpoch 1:  14%|█▍        | 14/100 [00:06<00:26,  3.24it/s]\u001b[A\nEpoch 1:  15%|█▌        | 15/100 [00:06<00:25,  3.37it/s]\u001b[A\nEpoch 1:  16%|█▌        | 16/100 [00:06<00:25,  3.31it/s]\u001b[A\nEpoch 1:  17%|█▋        | 17/100 [00:06<00:24,  3.44it/s]\u001b[A\nEpoch 1:  18%|█▊        | 18/100 [00:07<00:23,  3.53it/s]\u001b[A\nEpoch 1:  19%|█▉        | 19/100 [00:07<00:22,  3.60it/s]\u001b[A\nEpoch 1:  20%|██        | 20/100 [00:07<00:22,  3.49it/s]\u001b[A\nEpoch 1:  21%|██        | 21/100 [00:08<00:22,  3.49it/s]\u001b[A\nEpoch 1:  22%|██▏       | 22/100 [00:08<00:21,  3.62it/s]\u001b[A\nEpoch 1:  23%|██▎       | 23/100 [00:08<00:21,  3.65it/s]\u001b[A\nEpoch 1:  24%|██▍       | 24/100 [00:08<00:21,  3.50it/s]\u001b[A\nEpoch 1:  25%|██▌       | 25/100 [00:09<00:20,  3.58it/s]\u001b[A\nEpoch 1:  26%|██▌       | 26/100 [00:09<00:20,  3.63it/s]\u001b[A\nEpoch 1:  27%|██▋       | 27/100 [00:09<00:19,  3.67it/s]\u001b[A\nEpoch 1:  28%|██▊       | 28/100 [00:10<00:20,  3.54it/s]\u001b[A\nEpoch 1:  29%|██▉       | 29/100 [00:10<00:19,  3.63it/s]\u001b[A\nEpoch 1:  30%|███       | 30/100 [00:10<00:19,  3.67it/s]\u001b[A\nEpoch 1:  31%|███       | 31/100 [00:10<00:18,  3.71it/s]\u001b[A\nEpoch 1:  32%|███▏      | 32/100 [00:11<00:19,  3.56it/s]\u001b[A\nEpoch 1:  33%|███▎      | 33/100 [00:11<00:18,  3.68it/s]\u001b[A\nEpoch 1:  34%|███▍      | 34/100 [00:11<00:17,  3.74it/s]\u001b[A\nEpoch 1:  35%|███▌      | 35/100 [00:11<00:17,  3.73it/s]\u001b[A\nEpoch 1:  36%|███▌      | 36/100 [00:12<00:17,  3.57it/s]\u001b[A\nEpoch 1:  37%|███▋      | 37/100 [00:12<00:17,  3.62it/s]\u001b[A\nEpoch 1:  38%|███▊      | 38/100 [00:12<00:16,  3.68it/s]\u001b[A\nEpoch 1:  39%|███▉      | 39/100 [00:13<00:16,  3.71it/s]\u001b[A\nEpoch 1:  40%|████      | 40/100 [00:13<00:16,  3.56it/s]\u001b[A\nEpoch 1:  41%|████      | 41/100 [00:13<00:16,  3.62it/s]\u001b[A\nEpoch 1:  42%|████▏     | 42/100 [00:13<00:15,  3.65it/s]\u001b[A\nEpoch 1:  43%|████▎     | 43/100 [00:14<00:15,  3.71it/s]\u001b[A\nEpoch 1:  44%|████▍     | 44/100 [00:14<00:15,  3.57it/s]\u001b[A\nEpoch 1:  45%|████▌     | 45/100 [00:14<00:14,  3.67it/s]\u001b[A\nEpoch 1:  46%|████▌     | 46/100 [00:14<00:14,  3.73it/s]\u001b[A\nEpoch 1:  47%|████▋     | 47/100 [00:15<00:14,  3.77it/s]\u001b[A\nEpoch 1:  48%|████▊     | 48/100 [00:15<00:14,  3.61it/s]\u001b[A\nEpoch 1:  49%|████▉     | 49/100 [00:15<00:13,  3.71it/s]\u001b[A\nEpoch 1:  50%|█████     | 50/100 [00:15<00:13,  3.77it/s]\u001b[A\nEpoch 1:  51%|█████     | 51/100 [00:16<00:12,  3.79it/s]\u001b[A\nEpoch 1:  52%|█████▏    | 52/100 [00:16<00:13,  3.62it/s]\u001b[A\nEpoch 1:  53%|█████▎    | 53/100 [00:16<00:12,  3.73it/s]\u001b[A\nEpoch 1:  54%|█████▍    | 54/100 [00:17<00:12,  3.79it/s]\u001b[A\nEpoch 1:  55%|█████▌    | 55/100 [00:17<00:11,  3.86it/s]\u001b[A\nEpoch 1:  56%|█████▌    | 56/100 [00:17<00:11,  3.68it/s]\u001b[A\nEpoch 1:  57%|█████▋    | 57/100 [00:17<00:11,  3.72it/s]\u001b[A\nEpoch 1:  58%|█████▊    | 58/100 [00:18<00:11,  3.69it/s]\u001b[A\nEpoch 1:  59%|█████▉    | 59/100 [00:18<00:10,  3.77it/s]\u001b[A\nEpoch 1:  60%|██████    | 60/100 [00:18<00:11,  3.62it/s]\u001b[A\nEpoch 1:  61%|██████    | 61/100 [00:18<00:10,  3.71it/s]\u001b[A\nEpoch 1:  62%|██████▏   | 62/100 [00:19<00:10,  3.76it/s]\u001b[A\nEpoch 1:  63%|██████▎   | 63/100 [00:19<00:09,  3.80it/s]\u001b[A\nEpoch 1:  64%|██████▍   | 64/100 [00:19<00:09,  3.64it/s]\u001b[A\nEpoch 1:  65%|██████▌   | 65/100 [00:20<00:09,  3.73it/s]\u001b[A\nEpoch 1:  66%|██████▌   | 66/100 [00:20<00:09,  3.66it/s]\u001b[A\nEpoch 1:  67%|██████▋   | 67/100 [00:20<00:09,  3.64it/s]\u001b[A\nEpoch 1:  68%|██████▊   | 68/100 [00:20<00:09,  3.51it/s]\u001b[A\nEpoch 1:  69%|██████▉   | 69/100 [00:21<00:08,  3.64it/s]\u001b[A\nEpoch 1:  70%|███████   | 70/100 [00:21<00:07,  3.76it/s]\u001b[A\nEpoch 1:  71%|███████   | 71/100 [00:21<00:07,  3.81it/s]\u001b[A\nEpoch 1:  72%|███████▏  | 72/100 [00:21<00:07,  3.64it/s]\u001b[A\nEpoch 1:  73%|███████▎  | 73/100 [00:22<00:07,  3.72it/s]\u001b[A\nEpoch 1:  74%|███████▍  | 74/100 [00:22<00:06,  3.75it/s]\u001b[A\nEpoch 1:  75%|███████▌  | 75/100 [00:22<00:06,  3.78it/s]\u001b[A\nEpoch 1:  76%|███████▌  | 76/100 [00:23<00:06,  3.61it/s]\u001b[A\nEpoch 1:  77%|███████▋  | 77/100 [00:23<00:06,  3.69it/s]\u001b[A\nEpoch 1:  78%|███████▊  | 78/100 [00:23<00:05,  3.75it/s]\u001b[A\nEpoch 1:  79%|███████▉  | 79/100 [00:23<00:05,  3.78it/s]\u001b[A\nEpoch 1:  80%|████████  | 80/100 [00:24<00:05,  3.66it/s]\u001b[A\nEpoch 1:  81%|████████  | 81/100 [00:24<00:05,  3.76it/s]\u001b[A\nEpoch 1:  82%|████████▏ | 82/100 [00:24<00:04,  3.79it/s]\u001b[A\nEpoch 1:  83%|████████▎ | 83/100 [00:24<00:04,  3.80it/s]\u001b[A\nEpoch 1:  84%|████████▍ | 84/100 [00:25<00:04,  3.68it/s]\u001b[A\nEpoch 1:  85%|████████▌ | 85/100 [00:25<00:03,  3.78it/s]\u001b[A\nEpoch 1:  86%|████████▌ | 86/100 [00:25<00:03,  3.84it/s]\u001b[A\nEpoch 1:  87%|████████▋ | 87/100 [00:25<00:03,  3.84it/s]\u001b[A\nEpoch 1:  88%|████████▊ | 88/100 [00:26<00:03,  3.68it/s]\u001b[A\nEpoch 1:  89%|████████▉ | 89/100 [00:26<00:02,  3.76it/s]\u001b[A\nEpoch 1:  90%|█████████ | 90/100 [00:26<00:02,  3.80it/s]\u001b[A\nEpoch 1:  91%|█████████ | 91/100 [00:26<00:02,  3.82it/s]\u001b[A\nEpoch 1:  92%|█████████▏| 92/100 [00:27<00:02,  3.66it/s]\u001b[A\nEpoch 1:  93%|█████████▎| 93/100 [00:27<00:01,  3.75it/s]\u001b[A\nEpoch 1:  94%|█████████▍| 94/100 [00:27<00:01,  3.81it/s]\u001b[A\nEpoch 1:  95%|█████████▌| 95/100 [00:28<00:01,  3.71it/s]\u001b[A\nEpoch 1:  96%|█████████▌| 96/100 [00:28<00:01,  3.62it/s]\u001b[A\nEpoch 1:  97%|█████████▋| 97/100 [00:28<00:00,  3.70it/s]\u001b[A\nEpoch 1:  98%|█████████▊| 98/100 [00:28<00:00,  3.75it/s]\u001b[A\nEpoch 1:  99%|█████████▉| 99/100 [00:29<00:00,  3.77it/s]\u001b[A\nEpoch 1: 100%|██████████| 100/100 [00:29<00:00,  3.40it/s]\u001b[A\nEpoch 2: 100%|██████████| 100/100 [00:26<00:00,  3.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset-Tuned MolT5 Recommendation: The molecule is an organosilicon compound that is dimethylsilane in which one of the hydrogens attached to the silicon is replaced by a p-(methylthio)phenyl group, while the other is substituted by an ethylidene group. It is prone to cause immediate, severe inflammation of internal organs, including those associated with irritable bowel syndrome. Cilastatin is also used in the treatment of rheumatoid arthritis. it shows antitussive activity against primary and inflammation. Additionally it can be used as praziquantel for the alleviation of postoperative pain. Administered as the racemate, only the (S)-enantiomer is active.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.1 </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# Generate Morgan fingerprint for query and create graph data\ndef get_morgan_fingerprint_graph(smiles, radius=2, nBits=2048):\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return None\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits)\n        node_feat = torch.FloatTensor(list(fp)).unsqueeze(0)  # Convert to tensor with batch dim\n        edge_index = torch.tensor([[0], [0]], dtype=torch.long)  # Simple graph structure\n        return Data(x=node_feat, edge_index=edge_index)\n    except:\n        return None\n\n# Similarity search using HNSW with GIN embeddings\ndef search_similar_compounds(smiles, gin_model, index, k=5):\n    graph_data = get_morgan_fingerprint_graph(smiles)\n    if graph_data is None:\n        return None\n    graph_data = graph_data.to(device)\n    with torch.no_grad():\n        embedding = gin_model(graph_data.x, graph_data.edge_index, torch.zeros(1, dtype=torch.long).to(device))\n        print(f\"Embedding shape before reshape: {embedding.shape}\")  # Debug shape\n        # Reshape to 2D and convert to numpy\n        embedding = embedding.squeeze().cpu().numpy()  # Remove batch dim\n        query = embedding[np.newaxis, :]  # Add batch dimension for FAISS\n        print(f\"Query shape after reshape: {query.shape}\")  # Debug shape\n        if query.shape[1] != index.d:\n            raise ValueError(f\"Query dimension ({query.shape[1]}) does not match index dimension ({index.d})\")\n        distances, indices = index.search(query, k)  # Search with reshaped query\n    return df.iloc[indices[0]]['standard_smiles'].values\n\n# Fine-tune LLM with HNSW-derived compounds\ndef fine_tune_with_hnsw(model, tokenizer, smiles_list, similar_smiles, epochs=1, batch_size=1):\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n    scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n    # Determine model type using config\n    config = model.config\n    print(f\"Model config class: {config.__class__.__name__}\")  # Debug model type\n    if isinstance(config, PretrainedConfig) and hasattr(config, 'model_type'):\n        model_type = config.model_type\n        if model_type in ['biogpt', 'gpt2', 'llama']:  # Causal LM examples\n            model_type = 'causal'\n        elif model_type in ['t5', 'molt5']:  # Seq2Seq examples\n            model_type = 'seq2seq'\n        elif model_type in ['bert', 'roberta', 'chemberta']:  # Classification examples\n            model_type = 'classification'\n        else:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n    else:\n        raise ValueError(\"Unable to determine model type from configuration\")\n\n    # Create training examples combining dataset and HNSW similar compounds\n    templates = [\n        \"This compound, enhanced by similar structures, may exhibit anti-inflammatory properties.\",\n        \"A novel structure for drug development with possible antimicrobial effects based on similar compounds.\",\n        \"This chemical, informed by similar molecules, could be a new lead for cancer therapy research.\"\n    ]\n    train_data = [f\"{smiles}\\t{templates[i % len(templates)]}\" for i, smiles in enumerate(similar_smiles)]\n\n    for epoch in range(epochs):\n        np.random.shuffle(train_data)  # Shuffle to improve learning\n        optimizer.zero_grad()\n        for i in tqdm(range(0, len(train_data), batch_size), desc=f\"HNSW Tuning Epoch {epoch+1}\"):\n            batch = train_data[i:i+batch_size]\n            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n            \n            with torch.cuda.amp.autocast():\n                if model_type == 'causal':\n                    outputs = model(**inputs, labels=inputs['input_ids'])\n                elif model_type == 'seq2seq':\n                    decoder_input_ids = inputs['input_ids'].clone()\n                    decoder_input_ids[:, 1:] = decoder_input_ids[:, :-1].clone()  # Shift for teacher forcing\n                    decoder_input_ids[:, 0] = tokenizer.pad_token_id  # Start with pad token\n                    outputs = model(**inputs, decoder_input_ids=decoder_input_ids, labels=inputs['input_ids'])\n                elif model_type == 'classification':\n                    labels = torch.zeros(len(batch), dtype=torch.long).to(device)  # Dummy labels\n                    outputs = model(**inputs, labels=labels)\n                loss = outputs.loss  # No accumulation for now, adjust if needed\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n    model.eval()\n\n# Generate recommendation using HNSW-tuned LLM\ndef generate_hnsw_tuned_recommendation(model, tokenizer):\n    prompt = f\"Generate a novel chemical compound for drug development. Provide a SMILES string and its potential therapeutic application.\"\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n    \n    if hasattr(model.config, 'model_type') and model.config.model_type in ['biogpt', 'gpt2', 'llama']:\n        outputs = model.generate(\n            **inputs,\n            max_length=200,\n            num_return_sequences=1,\n            do_sample=True,\n            top_k=40,\n            top_p=0.92,\n            temperature=0.8,\n            no_repeat_ngram_size=2\n        )\n    elif hasattr(model.config, 'model_type') and model.config.model_type in ['t5', 'molt5']:\n        outputs = model.generate(\n            **inputs,\n            max_length=200,\n            num_return_sequences=1,\n            do_sample=True,\n            top_k=40,\n            top_p=0.92,\n            temperature=0.8,\n            no_repeat_ngram_size=2,\n            decoder_start_token_id=tokenizer.pad_token_id\n        )\n    elif hasattr(model.config, 'model_type') and model.config.model_type in ['bert', 'roberta', 'chemberta']:\n        return \"ChemBERTa is a classification model and cannot generate recommendations directly.\"\n    else:\n        raise ValueError(f\"Unsupported model type for generation: {model.config.model_type if hasattr(model.config, 'model_type') else 'Unknown'}\")\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n\nprint(\"HNSW TUNING COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:32:17.250121Z","iopub.execute_input":"2025-06-09T08:32:17.250462Z","iopub.status.idle":"2025-06-09T08:32:17.267071Z","shell.execute_reply.started":"2025-06-09T08:32:17.250444Z","shell.execute_reply":"2025-06-09T08:32:17.266336Z"}},"outputs":[{"name":"stdout","text":"HNSW TUNING COMPLETE\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"<div>\n    <h3 align=\"left\"><font color=\"red\"> R2.4 </font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"# HNSW-Tuned recommendation\nquery_smiles = df['standard_smiles'].iloc[0]  # Example query\nsimilar_smiles = search_similar_compounds(query_smiles, gin_model, index)\nhnsw_tuned_rec = None\nif similar_smiles is not None:\n    print(\"Similar Compounds:\")\n    for smi in similar_smiles:\n        print(smi)\n    # Re-tune LLM with HNSW-derived compounds\n    fine_tune_with_hnsw(llm_configs[active_llm]['model'], llm_configs[active_llm]['tokenizer'], df['standard_smiles'].tolist()[:100], similar_smiles)\n    hnsw_tuned_rec = generate_hnsw_tuned_recommendation(\n        llm_configs[active_llm]['model'], llm_configs[active_llm]['tokenizer']\n    )\n    print(f\"HNSW-Tuned {active_llm} Recommendation: {hnsw_tuned_rec}\")\nelse:\n    print(\"HNSW search failed due to invalid query SMILES or index mismatch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:32:17.267734Z","iopub.execute_input":"2025-06-09T08:32:17.267927Z","iopub.status.idle":"2025-06-09T08:32:20.867756Z","shell.execute_reply.started":"2025-06-09T08:32:17.267913Z","shell.execute_reply":"2025-06-09T08:32:20.866965Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/877543561.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()  # Enable mixed precision\n","output_type":"stream"},{"name":"stdout","text":"Embedding shape before reshape: torch.Size([1, 256])\nQuery shape after reshape: (1, 256)\nSimilar Compounds:\nO=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1\nNC(=O)c1cnc2ccccc2c1\nCC(NC(=O)c1c[nH]c2ncc(C3CC3)nc12)C1CCCCC1\nCOCc1ccc(O)c2ncccc12\nO=C1C2C3C=CC(C4C=CC43)C2C(=O)N1CCCCN1CCN(c2ncccn2)CC1\nModel config class: T5Config\n","output_type":"stream"},{"name":"stderr","text":"HNSW Tuning Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_35/877543561.py:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nHNSW Tuning Epoch 1: 100%|██████████| 5/5 [00:01<00:00,  3.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"HNSW-Tuned MolT5 Recommendation: The molecule is an organosilicon compound that is dimethylsilane in which one of the hydrogens attached to the silicon is replaced by a 1,2,4-triazol-1-yl group. It is metabolite of diazinon.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"<div>\n    <h4 align=\"left\"><font color=\"green\"> Saving Recommendations </font></h4>\n</div>","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/recommendations.txt', 'w') as f:\n    f.write(f\"Dataset-Tuned {active_llm} Recommendation: {dataset_tuned_rec}\\n\")\n    f.write(f\"HNSW-Tuned {active_llm} Recommendation: {hnsw_tuned_rec}\\n\")\nprint(\"Recommendations saved to /kaggle/working/recommendations.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T08:32:20.868602Z","iopub.execute_input":"2025-06-09T08:32:20.868958Z","iopub.status.idle":"2025-06-09T08:32:20.873710Z","shell.execute_reply.started":"2025-06-09T08:32:20.868935Z","shell.execute_reply":"2025-06-09T08:32:20.872928Z"}},"outputs":[{"name":"stdout","text":"Recommendations saved to /kaggle/working/recommendations.txt\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"<div>\n    <h2 align=\"center\"><font color=\"purple\"> Deliverable 3 Code </font></h2>\n</div>","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport ast\nimport re\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nimport faiss\nfrom torch_geometric.nn import GINConv, global_add_pool\nfrom torch_geometric.data import Data\nimport torch.nn as nn\nfrom langchain.prompts import ChatPromptTemplate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\nimport gc\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load preprocessed data\ndf = pd.read_csv('/kaggle/working/preprocessed_data_with_embeddings.csv')\nprint(\"Loaded preprocessed data\")\n\n# Convert gnn_embedding strings to arrays\ndef parse_embedding(embedding_str):\n    try:\n        return np.array(ast.literal_eval(embedding_str), dtype=np.float32)\n    except (ValueError, SyntaxError) as e:\n        print(f\"Error parsing embedding: {e}\")\n        return np.array([])\n\ndf['gnn_embedding'] = df['gnn_embedding'].apply(parse_embedding)\ndf = df[df['gnn_embedding'].apply(lambda x: len(x) > 0)]\nprint(\"Converted gnn_embedding strings to arrays\")\n\n# Load GNN model\nclass FingerprintGNN(nn.Module):\n    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=256):\n        super().__init__()\n        self.fp_to_node = nn.Linear(input_dim, hidden_dim)\n        self.conv1 = GINConv(nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n        self.conv2 = GINConv(nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n        self.lin = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, edge_index, batch):\n        x = self.fp_to_node(x)\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index)\n        return self.lin(global_add_pool(x, batch))\n\ngin_model = FingerprintGNN().to(device)\ngin_model.load_state_dict(torch.load('/kaggle/working/gin_model.pth'))\ngin_model.eval()\nprint(\"Loaded GNN model\")\n\n# Load HNSW index\nembedding_matrix = np.stack(df['gnn_embedding'].values)\nindex = faiss.read_index('/kaggle/working/gnn_hnsw_index.faiss')\nprint(\"Loaded HNSW index\")\n\n# Generate graph data from SMILES\ndef get_morgan_fingerprint_graph(smiles, radius=2, nBits=2048):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return None\n    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits)\n    node_feat = torch.FloatTensor(list(fp)).unsqueeze(0)\n    edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n    return Data(x=node_feat, edge_index=edge_index)\n\n# Validate SMILES and check novelty\ndef validate_smiles(smiles, context_smiles, dataset_smiles):\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return False, \"Invalid SMILES string\"\n        if smiles in context_smiles:\n            return False, \"Generated SMILES is not novel (matches context)\"\n        if smiles in dataset_smiles:\n            return False, \"Generated SMILES exists in dataset\"\n        return True, \"Valid and novel SMILES\"\n    except Exception as e:\n        return False, f\"Error processing SMILES: {str(e)}\"\n\n# RAG integration function\ndef rag_recommendation(llm_config, query_smiles, top_k=5, dataset_smiles=None):\n    graph_data = get_morgan_fingerprint_graph(query_smiles)\n    if graph_data is None:\n        return \"SMILES: Invalid Application: None\\nModel output issue: Invalid query SMILES\"\n    graph_data = graph_data.to(device)\n    with torch.no_grad():\n        query_embedding = gin_model(graph_data.x, graph_data.edge_index, torch.zeros(1, dtype=torch.long).to(device)).cpu().numpy()\n    print(f\"Query embedding shape: {query_embedding.shape}\")\n    if query_embedding.shape[1] != index.d:\n        raise ValueError(f\"Query dimension ({query_embedding.shape[1]}) does not match index dimension ({index.d})\")\n    query_embedding = query_embedding.reshape(1, -1).astype(np.float32)\n    faiss.normalize_L2(query_embedding)\n    \n    distances, indices = index.search(query_embedding, top_k)\n    similar_smiles = df.iloc[indices[0]]['standard_smiles'].values\n    context = \"\\n\".join([f\"Compound: {smi}\" for smi in similar_smiles])\n    prompt_template = ChatPromptTemplate.from_messages([\n        (\"system\", \"Output ONLY a novel, valid SMILES string and one therapeutic application in this format: SMILES: <smiles> Application: <application>. Do NOT repeat the prompt, include descriptions, extra text, or invalid SMILES. The SMILES must be valid and distinct from context compounds. Examples: SMILES: c1cc(c(c(c1)F)N)NC(=O)c2cnc(s2) Application: Potential use in treating Alzheimer’s disease; SMILES: c1cc(c(c(c1)OC)N)NC(=O)c2cnc(o2) Application: Potential use in treating HIV; SMILES: c1ccccc1C(=O)N Application: Potential use in treating epilepsy\"),\n        (\"user\", \"Context compounds:\\n{context}\\nOutput: SMILES: <smiles> Application: <application>\")\n    ])\n    \n    tokenizer = llm_config['tokenizer']\n    model = llm_config['model']\n    prompt = prompt_template.format(context=context)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=384).to(device)\n    \n    print(f\"Model type: {type(model)}\")\n    print(f\"Decoded prompt: {tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)}\")\n    \n    try:\n        with torch.no_grad():\n            if isinstance(model, AutoModelForCausalLM):\n                print(\"Detected as Causal LM\")\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=200,\n                    pad_token_id=tokenizer.pad_token_id,\n                    do_sample=True,\n                    temperature=0.3,\n                    top_p=0.9,\n                    top_k=50,\n                    no_repeat_ngram_size=2\n                )\n            elif isinstance(model, AutoModelForSeq2SeqLM):\n                print(\"Detected as Seq2Seq LM\")\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=200,\n                    pad_token_id=tokenizer.pad_token_id,\n                    do_sample=True,\n                    temperature=0.5,\n                    top_p=0.9,\n                    top_k=40,\n                    no_repeat_ngram_size=2,\n                    decoder_start_token_id=0\n                )\n            else:\n                raise ValueError(f\"Unsupported model type: {type(model)}\")\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Raw model output: {response}\")\n        \n        # Flexible parsing\n        smiles_match = re.search(r'(?:SMILES?|SMID:)\\s*([^\\n;]+)', response, re.IGNORECASE)\n        app_match = re.search(r'Application:\\s*([^\\n;]+)', response, re.IGNORECASE)\n        \n        if smiles_match and app_match:\n            smiles = smiles_match.group(1).strip()\n            application = app_match.group(1).strip()\n            is_valid, message = validate_smiles(smiles, similar_smiles, dataset_smiles)\n            if is_valid:\n                print(f\"Valid output: SMILES: {smiles} Application: {application}\")\n                return f\"SMILES: {smiles} Application: {application}\"\n            else:\n                print(f\"Validation failed: {message}\")\n                return f\"SMILES: Invalid Application: None\\nModel output issue: {message}\"\n        else:\n            error_msg = f\"Parsing failed: Missing {'SMILES' if not smiles_match else ''}{' and ' if not smiles_match and not app_match else ''}{'Application' if not app_match else ''}\"\n            print(error_msg)\n            return f\"SMILES: Invalid Application: None\\nModel output issue: {error_msg}\"\n    \n    except Exception as e:\n        error_msg = f\"Generation failed: {str(e)}\"\n        print(error_msg)\n        return f\"SMILES: Invalid Application: None\\nModel output issue: {error_msg}\"\n\n# LLM configurations\nllm_configs = {\n    'BioGPT': {\n        'model_name': 'microsoft/biogpt',\n        'tokenizer': AutoTokenizer.from_pretrained('microsoft/biogpt'),\n        'model': AutoModelForCausalLM.from_pretrained('microsoft/biogpt', torch_dtype=torch.float16).to(device)\n    },\n    'MolT5': {\n        'model_name': 'laituan245/molt5-large-smiles2caption',\n        'tokenizer': AutoTokenizer.from_pretrained('laituan245/molt5-large-smiles2caption'),\n        'model': AutoModelForSeq2SeqLM.from_pretrained('laituan245/molt5-large-smiles2caption', torch_dtype=torch.float16).to(device)\n    },\n    'T5-small': {\n        'model_name': 'google/t5-v1_1-small',\n        'tokenizer': AutoTokenizer.from_pretrained('google/t5-v1_1-small'),\n        'model': AutoModelForSeq2SeqLM.from_pretrained('google/t5-v1_1-small', torch_dtype=torch.float16).to(device)\n    }\n}\n\n# Execute models\nfor llm_name in llm_configs:\n    print(f\"Processing {llm_name} with RAG:...\")\n    query_smiles = df['standard_smiles'].iloc[0]\n    recommendation = rag_recommendation(llm_configs[llm_name], query_smiles, dataset_smiles=dataset_smiles)\n    print(f\"{llm_name} RAG Recommendation:\\n{recommendation}\\n\\n\")\n    # Clear memory\n    del llm_configs[llm_name]['model']\n    del llm_configs[llm_name]['tokenizer']\n    torch.cuda.empty_cache()\n    gc.collect()\n\nprint(\"Deliverable 3 code execution completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T09:11:10.052716Z","iopub.execute_input":"2025-06-09T09:11:10.053368Z","iopub.status.idle":"2025-06-09T09:11:32.246088Z","shell.execute_reply.started":"2025-06-09T09:11:10.053343Z","shell.execute_reply":"2025-06-09T09:11:32.245262Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded preprocessed data\nConverted gnn_embedding strings to arrays\nLoaded GNN model\nLoaded HNSW index\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c788ad94f3a4decace78b6055183f91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f07d81c2a5147fba8407e17f4c2b481"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26c7fbf0d524f66afdeb8fc41048e9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f55666df75e442b4a6bf7689756b1339"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72c6b4b9c57d4e79ad20fdcfe0f8c38a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ad755f27e84ca29155c83b35ea4dd7"}},"metadata":{}},{"name":"stdout","text":"Processing BioGPT with RAG:...\nQuery embedding shape: (1, 256)\nModel type: <class 'transformers.models.biogpt.modeling_biogpt.BioGptForCausalLM'>\nDecoded prompt: System: Output ONLY a novel, valid SMILES string and one therapeutic application in this format: SMILES: < smiles > Application: < application >. Do NOT repeat the prompt, include descriptions, extra text, or invalid SMILES. The SMILES must be valid and distinct from context compounds. Examples: SMILES: c1cc (c (c (c1) F) N) NC (= O) c2cnc (s2) Application: Potential use in treating Alzheimer s disease; SMILES: c1cc (c (c (c1) OC) N) NC (= O) c2cnc (o2) Application: Potential use in treating HIV; SMILES: c1ccccc1C (= O) N Application: Potential use in treating epilepsy Human: Context compounds: Compound: O = S (= O) (Nc1cccc (-c2cnc3ccccc3n2) c1) c1cccs1 Compound: NC (= O) c1cnc2ccccc2c1 Compound: CC (NC (= O) c1c [nH] c2ncc (C3CC3) nc12) C1CCCCC1 Compound: COCc1ccc (O) c2ncccc12 Compound: O = C1C2C3C = CC (C4C = CC43) C2C (= O) N1CCCCN1CCN (c2ncccn2) CC1 Output: SMILES: < smiles > Application: < application >\nGeneration failed: Unsupported model type: <class 'transformers.models.biogpt.modeling_biogpt.BioGptForCausalLM'>\nBioGPT RAG Recommendation:\nSMILES: Invalid Application: None\nModel output issue: Generation failed: Unsupported model type: <class 'transformers.models.biogpt.modeling_biogpt.BioGptForCausalLM'>\n\n\nProcessing MolT5 with RAG:...\nQuery embedding shape: (1, 256)\nModel type: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\nDecoded prompt: System: Output ONLY a novel, valid SMILES string and one therapeutic application in this format: SMILES: smiles> Application: application>. Do NOT repeat the prompt, include descriptions, extra text, or invalid SMILES. The SMILES must be valid and distinct from context compounds. Examples: SMILES: c1cc(c(c(c1)F)N)NC(=O)c2cnc(s2) Application: Potential use in treating Alzheimer’s disease; SMILES: c1cc(c(c(c1)OC)N)NC(=O)c2cnc(o2) Application: Potential use in treating HIV; SMILES: c1ccccc1C(=O)N Application: Potential use in treating epilepsy Human: Context compounds: Compound: O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1 Compound: NC(=O)c1cnc2ccccc2c1 Compound: CC(NC(=O)c1c[nH]c2ncc(C3CC3)nc12)C1CCCCC1 Compound: COCc1ccc(O)c2ncccc12 Compound: O=C1C2C3C=CC(C4C=CC43)C2C(=O)N1CCCCN1CCN(c2ncccn2)CC1 Output: SMILES: smiles> Application: application>\nGeneration failed: Unsupported model type: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\nMolT5 RAG Recommendation:\nSMILES: Invalid Application: None\nModel output issue: Generation failed: Unsupported model type: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf69462805ca440386f1ab6285c58f92"}},"metadata":{}},{"name":"stdout","text":"Processing T5-small with RAG:...\nQuery embedding shape: (1, 256)\nModel type: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\nDecoded prompt: System: Output ONLY a novel, valid SMILES string and one therapeutic application in this format: SMILES: smiles> Application: application>. Do NOT repeat the prompt, include descriptions, extra text, or invalid SMILES. The SMILES must be valid and distinct from context compounds. Examples: SMILES: c1cc(c(c(c1)F)N)NC(=O)c2cnc(s2) Application: Potential use in treating Alzheimer’s disease; SMILES: c1cc(c(c(c1)OC)N)NC(=O)c2cnc(o2) Application: Potential use in treating HIV; SMILES: c1ccccc1C(=O)N Application: Potential use in treating epilepsy Human: Context compounds: Compound: O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1 Compound: NC(=O)c1cnc2ccccc2c1 Compound: CC(NC(=O)c1c[nH]c2ncc(C3CC3)nc12)C1CCCCC1 Compound: COCc1ccc(O)c2ncccc12 Compound: O=C1C2C3C=CC(C4C=CC43)C2C(=O)N1CCCCN1CCN(c2ncccn2)CC1 Output: SMILES: smiles> Application: application>\nGeneration failed: Unsupported model type: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\nT5-small RAG Recommendation:\nSMILES: Invalid Application: None\nModel output issue: Generation failed: Unsupported model type: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n\n\nDeliverable 3 code execution completed\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null}]}
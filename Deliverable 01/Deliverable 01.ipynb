{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 align=\"center\"><font color=\"blue\"> DELIVERABLE 2 </font></h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4 align=\"left\"><font color=\"green\"> Downloading Libraries </font></h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-07T22:01:09.364285Z",
     "iopub.status.busy": "2025-06-07T22:01:09.364032Z",
     "iopub.status.idle": "2025-06-07T22:02:41.246891Z",
     "shell.execute_reply": "2025-06-07T22:02:41.246157Z",
     "shell.execute_reply.started": "2025-06-07T22:01:09.364265Z"
    },
    "id": "dXsSaIFHPEAc",
    "outputId": "5667d7dd-a341-4c9e-a923-38a5ae4af8c8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit-pypi\n",
      "  Downloading rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (11.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.18)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit-pypi) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit-pypi) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit-pypi) (2024.2.0)\n",
      "Downloading rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sacremoses, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch_geometric, rdkit-pypi, faiss-cpu, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "Successfully installed bitsandbytes-0.46.0 faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rdkit-pypi-2022.9.5 sacremoses-0.1.1 torch_geometric-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rdkit-pypi torch_geometric faiss-cpu sacremoses bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:02:41.248556Z",
     "iopub.status.busy": "2025-06-07T22:02:41.248326Z",
     "iopub.status.idle": "2025-06-07T22:02:41.252901Z",
     "shell.execute_reply": "2025-06-07T22:02:41.252241Z",
     "shell.execute_reply.started": "2025-06-07T22:02:41.248532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- ALL LIBRARIES HAVE BEEN DOWNLOADED ----------\n"
     ]
    }
   ],
   "source": [
    "# rdkit-pypi: Helps me work with chemical structures and SMILES strings for molecules.\n",
    "# torch_geometric: Allows me to build graph neural networks (GNNs) for processing molecular data.\n",
    "# faiss-cpu: Used for fast similarity searches with embeddings, like finding similar compounds.\n",
    "# sacremoses: Likely needed for text processing, possibly for the language model part.\n",
    "# bitsandbytes: Helps with memory-efficient model training, especially for large language models.\n",
    "                                             \n",
    "print(\"---------- ALL LIBRARIES HAVE BEEN DOWNLOADED ----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4 align=\"left\"><font color=\"green\"> Importing Libraries </font></h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:02:41.254011Z",
     "iopub.status.busy": "2025-06-07T22:02:41.253655Z",
     "iopub.status.idle": "2025-06-07T22:02:53.518178Z",
     "shell.execute_reply": "2025-06-07T22:02:53.517539Z",
     "shell.execute_reply.started": "2025-06-07T22:02:41.253988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- ALL LIBRARIES HAVE BEEN IMPORTED ----------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "import faiss\n",
    "\n",
    "print(\"---------- ALL LIBRARIES HAVE BEEN IMPORTED ----------\")\n",
    "\n",
    "# torch, torch.nn, and torch.nn.functional: For building and training neural networks, like my GNN model.\n",
    "# torch_geometric modules (GINConv, global_add_pool, Data, Batch, DataLoader): Help me create and process graph-based data for molecules.\n",
    "# rdkit modules (Chem, AllChem, DataStructs, Descriptors): lets me work with chemical structures, generate fingerprints, and calculate properties like logP.\n",
    "# faiss: For efficient similarity searches using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:02:53.520017Z",
     "iopub.status.busy": "2025-06-07T22:02:53.519577Z",
     "iopub.status.idle": "2025-06-07T22:02:53.524175Z",
     "shell.execute_reply": "2025-06-07T22:02:53.523358Z",
     "shell.execute_reply.started": "2025-06-07T22:02:53.519995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h3 align=\"left\"><font color=\"red\"> STEP 01: Data Loading and Preprocessing </font></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:02:53.525424Z",
     "iopub.status.busy": "2025-06-07T22:02:53.525133Z",
     "iopub.status.idle": "2025-06-07T22:02:58.580072Z",
     "shell.execute_reply": "2025-06-07T22:02:58.579434Z",
     "shell.execute_reply.started": "2025-06-07T22:02:53.525399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['SMILES', 'pIC50', 'mol', 'num_atoms', 'logP']\n",
      "Removed 0 invalid SMILES strings.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/smiles/SMILES_Big_Data_Set.csv')\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "\n",
    "# Standardizing SMILES strings to ensure consistency and track invalid ones.\n",
    "invalid_smiles_count = 0\n",
    "def standardize_smiles(smiles):\n",
    "    global invalid_smiles_count\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule object.\n",
    "        if mol is None:\n",
    "            invalid_smiles_count += 1 \n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, isomericSmiles=True)  # Convert back to standardized SMILES.\n",
    "    except:\n",
    "        invalid_smiles_count += 1  # Increment counter if conversion fails.\n",
    "        return None\n",
    "\n",
    "df['standard_smiles'] = df['SMILES'].apply(standardize_smiles) \n",
    "df = df.dropna(subset=['standard_smiles']).drop_duplicates(subset=['standard_smiles'])\n",
    "print(f\"Removed {invalid_smiles_count} invalid SMILES strings.\")\n",
    "\n",
    "\n",
    "df['pIC50'] = pd.to_numeric(df['pIC50'], errors='coerce') \n",
    "df['num_atoms'] = pd.to_numeric(df['num_atoms'], errors='coerce')  \n",
    "df['logP'] = pd.to_numeric(df['logP'], errors='coerce') \n",
    "df = df.dropna() \n",
    "\n",
    "# Creating a column of RDKit molecule objects for later use, like generating fingerprints.\n",
    "df['mol'] = df['standard_smiles'].apply(Chem.MolFromSmiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h3 align=\"left\"><font color=\"red\"> STEP 02: Generating Fingerprints (Morgan Fingerprints) </font></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-07T22:02:58.581191Z",
     "iopub.status.busy": "2025-06-07T22:02:58.580822Z",
     "iopub.status.idle": "2025-06-07T22:03:00.310423Z",
     "shell.execute_reply": "2025-06-07T22:03:00.309743Z",
     "shell.execute_reply.started": "2025-06-07T22:02:58.581171Z"
    },
    "id": "iHGFFs4kPEAd",
    "outputId": "152d69c8-405a-4e3d-fb0e-36047bb5b921",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprint matrix shape: (14823, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Creating Morgan fingerprints to represent molecular structures numerically for GNN input.\n",
    "def generate_morgan_fingerprint(mol, radius=2, n_bits=2048):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)  # Generate 2048-bit Morgan fingerprint with radius 2.\n",
    "        arr = np.zeros((n_bits,), dtype=np.float32)\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)  # Convert fingerprint to NumPy array of 0s and 1s.\n",
    "        return arr\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['morgan_fp'] = df['mol'].apply(generate_morgan_fingerprint)  \n",
    "df = df[df['morgan_fp'].notnull()]  # Remove rows where fingerprint generation failed.\n",
    "fp_matrix = np.stack(df['morgan_fp'].values)  # Stack all fingerprints into a single NumPy array for GNN training.\n",
    "print(f\"Fingerprint matrix shape: {fp_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h3 align=\"left\"><font color=\"red\"> STEP 03: GNN for Fingerprint Embedding (GIN) </font></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:03:00.311376Z",
     "iopub.status.busy": "2025-06-07T22:03:00.311133Z",
     "iopub.status.idle": "2025-06-07T22:03:09.733088Z",
     "shell.execute_reply": "2025-06-07T22:03:09.732461Z",
     "shell.execute_reply.started": "2025-06-07T22:03:00.311357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GIN model...\n",
      "Epoch 1, Loss: 0.0014479051144384168\n",
      "Epoch 2, Loss: 0.0004800298243183\n",
      "Epoch 3, Loss: 0.0003346442300143876\n",
      "Epoch 4, Loss: 0.0003052238546309848\n",
      "Epoch 5, Loss: 0.0003707754834161686\n",
      "Epoch 6, Loss: 0.0004837360282745694\n",
      "Epoch 7, Loss: 0.0008593910772168752\n",
      "Epoch 8, Loss: 0.002141221021984479\n",
      "Epoch 9, Loss: 0.001700232632895771\n",
      "Epoch 10, Loss: 0.0017588131043433759\n",
      "\n",
      "Generating GNN embeddings...\n",
      "Embedding matrix shape: (14823, 256)\n"
     ]
    }
   ],
   "source": [
    "# Defining a Graph Neural Network (GNN) to create compact embeddings from Morgan fingerprints.\n",
    "class FingerprintGNN(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.fp_to_node = nn.Linear(input_dim, hidden_dim)  # Reduce 2048-bit fingerprint to 512 dimensions.\n",
    "        self.conv1 = GINConv(nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # First linear layer for graph convolution.\n",
    "            nn.ReLU(),  # Activation\n",
    "            nn.Linear(hidden_dim, hidden_dim)  # Second linear layer for feature transformation.\n",
    "        ))\n",
    "        self.conv2 = GINConv(nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Second graph convolution layer.\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ))\n",
    "        self.lin = nn.Linear(hidden_dim, output_dim)  # Final layer to output 256-dimensional embedding.\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.fp_to_node(x)  # Transform input fingerprint to hidden dimension.\n",
    "        x = self.conv1(x, edge_index).relu() \n",
    "        x = self.conv2(x, edge_index) \n",
    "        pooled = global_add_pool(x, batch)  # Aggregate node features into a single embedding per graph.\n",
    "        return self.lin(pooled)  \n",
    "\n",
    "data_list = []\n",
    "for fp in df['morgan_fp']:\n",
    "    node_feat = torch.FloatTensor(fp).unsqueeze(0)  # Convert fingerprint to tensor and add batch dimension.\n",
    "    edge_index = torch.tensor([[0], [0]], dtype=torch.long) \n",
    "    data = Data(x=node_feat, edge_index=edge_index) \n",
    "    data_list.append(data)\n",
    "\n",
    "batch_size = 128  # Set batch size for efficient training.\n",
    "loader = DataLoader(data_list, batch_size=batch_size, shuffle=False)  # Create DataLoader for batching graphs.\n",
    "\n",
    "# Training the GNN model using an autoencoder-like loss.\n",
    "gin_model = FingerprintGNN().to(device)  \n",
    "optimizer = torch.optim.Adam(gin_model.parameters(), lr=0.001)  # Set up Adam optimizer.\n",
    "target_projection = nn.Linear(2048, 256).to(device)  # Linear layer to project fingerprints to 256 dimensions for loss calculation.\n",
    "\n",
    "# Ensure=ing target_projection parameters are optimized along with GNN.\n",
    "combined_params = list(gin_model.parameters()) + list(target_projection.parameters())\n",
    "optimizer = torch.optim.Adam(combined_params, lr=0.001) \n",
    "\n",
    "epochs = 10\n",
    "\n",
    "print(\"\\nTraining GIN model...\")\n",
    "for epoch in range(epochs):\n",
    "    gin_model.train()  \n",
    "    target_projection.train() \n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = gin_model(batch.x, batch.edge_index, batch.batch)  # Get GNN embeddings.\n",
    "        target = target_projection(batch.x) \n",
    "        loss = F.mse_loss(out, target)  # Calculate MSE loss between GNN and projected embeddings.\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update model weights.\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader)}\")\n",
    "\n",
    "# Generating embeddings for all fingerprints using the trained GNN.\n",
    "print(\"\\nGenerating GNN embeddings...\")\n",
    "gin_model.eval()  \n",
    "target_projection.eval() \n",
    "embeddings = []\n",
    "with torch.no_grad():  # Disable gradient tracking to save memory.\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device) \n",
    "        emb = gin_model(batch.x, batch.edge_index, batch.batch)  # Generate embeddings.\n",
    "        embeddings.append(emb.cpu().numpy()) \n",
    "embedding_matrix = np.vstack(embeddings) \n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4 align=\"left\"><font color=\"green\"> Saving preprocessed data, embeddings, trained model </font></h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:03:09.734037Z",
     "iopub.status.busy": "2025-06-07T22:03:09.733784Z",
     "iopub.status.idle": "2025-06-07T22:03:16.292285Z",
     "shell.execute_reply": "2025-06-07T22:03:16.291629Z",
     "shell.execute_reply.started": "2025-06-07T22:03:09.734009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved!\n"
     ]
    }
   ],
   "source": [
    "# Saving my processed data and trained GNN model for later use.\n",
    "df['gnn_embedding'] = embedding_matrix.tolist() \n",
    "df.to_csv('preprocessed_data_with_embeddings.csv', index=False) \n",
    "\n",
    "# Saving the GNN model's weights to a file.\n",
    "torch.save(gin_model.state_dict(), \"gin_model.pth\") \n",
    "\n",
    "print(\"Data Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4 align=\"left\"><font color=\"green\"> Checking if required columns exist in df </font></h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:03:16.293369Z",
     "iopub.status.busy": "2025-06-07T22:03:16.293080Z",
     "iopub.status.idle": "2025-06-07T22:03:16.301897Z",
     "shell.execute_reply": "2025-06-07T22:03:16.301297Z",
     "shell.execute_reply.started": "2025-06-07T22:03:16.293343Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Columns Exist!\n"
     ]
    }
   ],
   "source": [
    "# Checking if my DataFrame has the necessary columns for later steps.\n",
    "if 'gnn_embedding' not in df.columns or 'standard_smiles' not in df.columns:\n",
    "    raise ValueError(\"Required columns 'gnn_embedding' or 'standard_smiles' not found in DataFrame.\")\n",
    "else:\n",
    "    print(\"Required Columns Exist!\")\n",
    "\n",
    "# Resetting the DataFrame index to align with the embedding matrix.\n",
    "df = df.reset_index(drop=True)  # Ensure row indices match embedding matrix to avoid mismatches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h3 align=\"left\"><font color=\"red\"> STEP 04: HNSW Index for GNN Embeddings </font></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:03:16.304084Z",
     "iopub.status.busy": "2025-06-07T22:03:16.303645Z",
     "iopub.status.idle": "2025-06-07T22:03:54.751936Z",
     "shell.execute_reply": "2025-06-07T22:03:54.751072Z",
     "shell.execute_reply.started": "2025-06-07T22:03:16.304066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 14823 compounds.\n"
     ]
    }
   ],
   "source": [
    "# Converting GNN embeddings to a NumPy array for Faiss.\n",
    "embedding_matrix = np.stack(df['gnn_embedding'].values).astype(np.float32)  \n",
    "embedding_dim = embedding_matrix.shape[1] \n",
    "\n",
    "index = faiss.IndexHNSWFlat(embedding_dim, 32)  # Create HNSW index with M=32 (graph degree).\n",
    "index.hnsw.efConstruction = 200  # Set construction parameter for better index quality.\n",
    "index.hnsw.efSearch = 100  # Set search parameter for better accuracy.\n",
    "faiss.normalize_L2(embedding_matrix)  # Normalize embeddings for cosine similarity.\n",
    "\n",
    "index.add(embedding_matrix)  # Index all embeddings for similarity searches.\n",
    "print(f\"Indexed {embedding_matrix.shape[0]} compounds.\")\n",
    "\n",
    "# Saving the index to a file for later use.\n",
    "faiss.write_index(index, \"gnn_hnsw_index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h3 align=\"left\"><font color=\"red\"> STEP 05: HNSW Search Function </font></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:03:54.753285Z",
     "iopub.status.busy": "2025-06-07T22:03:54.752977Z",
     "iopub.status.idle": "2025-06-07T22:03:54.760351Z",
     "shell.execute_reply": "2025-06-07T22:03:54.759599Z",
     "shell.execute_reply.started": "2025-06-07T22:03:54.753260Z"
    },
    "id": "gKO3krgLPEAe",
    "outputId": "ec6c4291-c092-40d0-99eb-7331b4151e2f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Compound Search Function made!\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to find compounds similar to a query fingerprint using the HNSW index.\n",
    "def search_similar_compounds(query_fp, gin_model, index, top_k=5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Search for compounds similar to the query fingerprint using HNSW index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Setting up the GNN model to generate embeddings for the query.\n",
    "        gin_model.eval() \n",
    "        gin_model.to(device) \n",
    "\n",
    "        query_fp = np.array(query_fp, dtype=np.float32)  \n",
    "        node_feat = torch.FloatTensor(query_fp).unsqueeze(0).to(device) \n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long).to(device)  # Create self-loop for single-node graph.\n",
    "        data = Data(x=node_feat, edge_index=edge_index)  # Wrap in Data object.\n",
    "        batch = torch.zeros(1, dtype=torch.long).to(device)  # Batch tensor for single graph.\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            query_embedding = gin_model(data.x, data.edge_index, batch).cpu().numpy()  # Get 256-dimensional embedding.\n",
    "        \n",
    "        query_embedding = query_embedding.astype(np.float32) \n",
    "        faiss.normalize_L2(query_embedding)\n",
    "\n",
    "        # Searching for the top_k most similar compounds.\n",
    "        _, indices = index.search(query_embedding, top_k)  \n",
    "\n",
    "        # Retrieving the SMILES strings of similar compounds.\n",
    "        similar_smiles = df.iloc[indices[0]]['standard_smiles'].values.tolist() \n",
    "        return similar_smiles\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "        return []  \n",
    "\n",
    "print(\"Similar Compound Search Function made!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h4 align=\"left\"><font color=\"green\"> Example Search Using HNSW </font></h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T22:03:54.761389Z",
     "iopub.status.busy": "2025-06-07T22:03:54.761159Z",
     "iopub.status.idle": "2025-06-07T22:03:54.788525Z",
     "shell.execute_reply": "2025-06-07T22:03:54.787834Z",
     "shell.execute_reply.started": "2025-06-07T22:03:54.761362Z"
    },
    "id": "T39I6YSRPEAf",
    "outputId": "4e5f57a5-38c5-4a56-fa13-0e678033614e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching for similar compounds...\n",
      "\n",
      "Top 5 Similar Compounds:\n",
      "1. Cc1ccc2cccnc2c1\n",
      "2. C=CC12COC(=O)C(=C)C1C1OC(=O)C(=C)C1C(O)C2\n",
      "3. Nc1ncnc2ncn(C(c3ccccc3)c3ccccc3)c12\n",
      "4. CC(C)CC(N)C(=O)NC(CC(C)C)C(=O)NC(C)C(=O)NC(Cc1ccccc1)C(=O)O\n",
      "5. COC(=O)C(C)NP(=O)(OCC1C=CC(n2cc(C)c(=O)[nH]c2=O)O1)Oc1cccc(I)c1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSearching for similar compounds...\")\n",
    "\n",
    "# Testing the similarity search with a sample SMILES string.\n",
    "query_smiles = \"NS(=O)(=O)N1CCC(NC(=O)c2cnn3ccc(N4CCCC4c4cc(F)ccc4F)nc23)CC1\"\n",
    "query_mol = Chem.MolFromSmiles(query_smiles)  # Convert SMILES to RDKit molecule.\n",
    "if query_mol is None:\n",
    "    print(\"Error: Invalid query SMILES string.\")\n",
    "else:\n",
    "    query_fp = generate_morgan_fingerprint(query_mol)  # Generate Morgan fingerprint for query.\n",
    "    if query_fp is None:\n",
    "        print(\"Error: Failed to generate fingerprint for query molecule.\")\n",
    "    else:\n",
    "        # Using the search function to find similar compounds.\n",
    "        similar_compounds = search_similar_compounds(query_fp, gin_model, index, top_k=5, device=device)  # Find top 5 similar compounds.\n",
    "        print(\"\\nTop 5 Similar Compounds:\")\n",
    "        for i, smiles in enumerate(similar_compounds, 1):\n",
    "            print(f\"{i}. {smiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7200232,
     "sourceId": 11487321,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7509207,
     "sourceId": 11944874,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

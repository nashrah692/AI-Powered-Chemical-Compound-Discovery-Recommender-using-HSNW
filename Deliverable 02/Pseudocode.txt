Algorithm: Chemical Compound Recommendation System

Input: Dataset with SMILES, Morgan fingerprints, logP, and GNN embeddings; Query SMILES string
Output: List of similar compounds and novel compound recommendations for MolT5, ChemBERTa, BioGPT

1. Preprocessing
   - Load SMILES dataset
   - Generate Morgan fingerprints
   - Compute GNN embeddings
   - Save preprocessed dataset with SMILES, fingerprints, logP, embeddings

2. Load preprocessed dataset with SMILES, Morgan fingerprints, logP, and GNN embeddings
3. Parse Morgan fingerprints from string to array
4. Load pre-trained HNSW index
5. Define GIN model for fingerprint embedding
6. Load GIN model weights
7. Initialize CompoundRecommender with HNSW index, dataset, and GIN model

Procedure TuneOnDataset(model_name, epochs, batch_size, grad_accum_steps)
8. Load LLM model and tokenizer with given model_name (MolT5, ChemBERTa, BioGPT)
9. If model or tokenizer not loaded then
10.    Return error message
11. End if
12. Generate prompts for 100 random SMILES with templates (e.g., "This compound may exhibit anti-inflammatory properties")
13. Set model to training mode
14. Initialize Adam optimizer (learning_rate=1e-5)
15. Enable mixed precision training with GradScaler
16. Prepare dataset and loader for tokenized prompts (max_length=128)
17. For each epoch from 1 to epochs do
18.    Initialize total_loss and step
19.    Clear gradients
20.    For each batch in loader do
21.        Move batch to device (cuda)
22.        Compute loss with gradient accumulation
23.        If loss is not NaN then
24.            Backpropagate loss
25.            Clip gradients (max_norm=1.0)
26.            Increment step
27.            If step mod grad_accum_steps equals 0 then
28.                Update weights
29.                Clear gradients
30.            End if
31.            Add loss to total_loss
32.        End if
33.    End for
34.    If step mod grad_accum_steps not zero then
35.        Update weights
36.        Clear gradients
37.    End if
38.    Print epoch and average loss
39. End for
40. Save fine-tuned model
41. Clear model and free memory
42. Print tuning completion
End procedure

Procedure TuneWithHNSW(model_name, epochs, batch_size, grad_accum_steps)
43. Load LLM model and tokenizer with given model_name (MolT5, ChemBERTa, BioGPT)
44. If model or tokenizer not loaded then
45.    Return error message
46. End if
47. Generate pairs of similar compounds using HNSW index (top-5 similar SMILES per query)
48. Generate comparison prompts for pairs (e.g., "This compound, similar to [SMILES], has antimicrobial potential")
49. Set model to training mode
50. Initialize Adam optimizer (learning_rate=1e-5)
51. Enable mixed precision training with GradScaler
52. Prepare dataset and loader for tokenized prompts (max_length=128)
53. For each epoch from 1 to epochs do
54.    Initialize total_loss and step
55.    Clear gradients
56.    For each batch in loader do
57.        Move batch to device (cuda)
58.        Compute loss with gradient accumulation
59.        If loss is not NaN then
60.            Backpropagate loss
61.            Clip gradients (max_norm=1.0)
62.            Increment step
63.            If step mod grad_accum_steps equals 0 then
64.                Update weights
65.                Clear gradients
66.            End if
67.            Add loss to total_loss
68.        End if
69.    End for
70.    If step mod grad_accum_steps not zero then
71.        Update weights
72.        Clear gradients
73.    End if
74.    Print epoch and average loss
75. End for
76. Save fine-tuned model
77. Clear model and free memory
78. Print tuning completion
End procedure

Function FPToEmbedding(fingerprint)
79. Prepare fingerprint as graph data for GIN model
80. Set GIN model to evaluation mode
81. Generate 256-dimensional embedding using GIN model
82. Return embedding
End function

Function Recommend(query_smiles, model_name, top_k)
83. Convert query SMILES to molecule using RDKit
84. If molecule is None then
85.    Return error message
86. End if
87. Generate Morgan fingerprint for query SMILES
88. Convert fingerprint to embedding using FPToEmbedding
89. Normalize embedding for cosine similarity
90. Search HNSW index for top_k similar compounds
91. Get SMILES and logP of similar compounds
92. Compute Tanimoto similarities for similar compounds
93. Load LLM model and tokenizer with given model_name (MolT5, ChemBERTa, BioGPT)
94. If model or tokenizer not loaded then
95.    Return error message
96. End if
97. If model_name is ChemBERTa then
98.    Return warning: "ChemBERTa cannot generate recommendations"
99. End if
100. Prepare prompt with query SMILES and similar compounds (e.g., "Generate a novel compound similar to [SMILES]")
101. Generate recommendation using LLM (max_length=200, top_k=40, top_p=0.92, temperature=0.8)
102. Extract SMILES, Tanimoto similarity, logP, and therapeutic reasoning from output
103. If extraction fails then
104.    Modify query SMILES with solubility-enhancing group
105.    Compute new Tanimoto similarity and logP
106.    Set reasoning to solubility enhancement
107. End if
108. Clear model and free memory
109. Return similar compounds and recommendation (SMILES, logP, reasoning)
End function

110. Initialize CompoundRecommender
111. Set query SMILES
112. For each model_name in {MolT5, ChemBERTa, BioGPT} do
113.    Call TuneOnDataset(model_name, epochs=2, batch_size=1, grad_accum_steps=4)
114.    Call Recommend(query_smiles, model_name, top_k=5)
115.    Save dataset-tuned recommendation to recommendations.txt
116.    Call TuneWithHNSW(model_name, epochs=1, batch_size=1, grad_accum_steps=4)
117.    Call Recommend(query_smiles, model_name, top_k=5)
118.    Save HNSW-tuned recommendation to recommendations.txt
119. End for
120. Generate final recommendation by selecting best output (highest Tanimoto similarity or logP) from MolT5 or BioGPT
121. Save final recommendation to recommendations.txt